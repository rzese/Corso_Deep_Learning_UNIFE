{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03_Intro_NN_exercises.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Q9V45VKOZFei","colab_type":"text"},"source":["# Intro to Neural Networks\n","\n","## Exercise: neurons as logic gates\n","In this exercise we will experiment with neuron computations.  We will show how to represent basic logic functions like AND, OR, and XOR using single neurons (or more complicated structures).  Finally, at the end we will walk through how to represent neural networks as a chain of matrix computations."]},{"cell_type":"code","metadata":{"id":"X353-7ErZFek","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sdX4jZY9ZFep","colab_type":"text"},"source":["### Sigmoid function:\n","\n","$$\n","\\sigma = \\frac{1}{1 + e^{-x}}\n","$$\n","\n","$\\sigma$ ranges from (0, 1). When the input $x$ is negative, $\\sigma$ is close to 0. When $x$ is positive, $\\sigma$ is close to 1. At $x=0$, $\\sigma=0.5$"]},{"cell_type":"code","metadata":{"id":"55-go285ZFeq","colab_type":"code","colab":{}},"source":["## Quickly define the sigmoid function\n","def sigmoid(x):\n","    \"\"\"Sigmoid function\"\"\"\n","    return 1.0 / (1.0 + np.exp(-x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b40xpSlMZFeu","colab_type":"code","colab":{}},"source":["# Plot the sigmoid function\n","vals = np.linspace(-10, 10, num=100, dtype=np.float32)\n","activation = sigmoid(vals)\n","fig = plt.figure(figsize=(12,6))\n","fig.suptitle('Sigmoid function')\n","plt.plot(vals, activation)\n","plt.grid(True, which='both')\n","plt.axhline(y=0, color='k')\n","plt.axvline(x=0, color='k')\n","plt.yticks() # forces the presence of values on y axis\n","plt.ylim([-0.5, 1.5]);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NGa-Y90yZFe0","colab_type":"text"},"source":["### Thinking of neurons as boolean logic gates\n","\n","A logic gate takes in two boolean (true/false or 1/0) inputs, and returns either a 0 or 1 depending on its rule. The truth table for a logic gate shows the outputs for each combination of inputs, (0, 0), (0, 1), (1,0), and (1, 1). For example, let's look at the truth table for an \"OR\" gate:\n","\n","### OR Gate\n","\n","<table>\n","\n","<tr>\n","<th colspan=\"3\">OR gate truth table</th>\n","</tr>\n","\n","<tr>\n","<th colspan=\"2\">Input</th>\n","<th>Output</th>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>0</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>1</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>0</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>1</td>\n","<td>1</td>\n","</tr>\n","\n","</table>\n","\n","A neuron that uses the sigmoid activation function outputs a value between (0, 1). This naturally leads us to think about boolean values. Imagine a neuron that takes in two inputs, $x_1$ and $x_2$, and a bias term.\n","\n","By limiting the inputs of $x_1$ and $x_2$ to be in $\\left\\{0, 1\\right\\}$, we can simulate the effect of logic gates with our neuron. The goal is to find the weights (represented by ? marks above), such that it returns an output close to 0 or 1 depending on the inputs.\n","\n","What numbers for the weights would we need to fill in for this gate to output OR logic? Observe from the plot above that $\\sigma(z)$ is close to 0 when $z$ is largely negative (around -10 or less), and is close to 1 when $z$ is largely positive (around +10 or greater).\n","\n","$$\n","z = w_1 x_1 + w_2 x_2 + b\n","$$\n","\n","Let's think this through:\n","\n","* When $x_1$ and $x_2$ are both 0, the only value affecting $z$ is $b$. Because we want the result for (0, 0) to be close to zero, $b$ should be negative (at least -10)\n","* If either $x_1$ or $x_2$ is 1, we want the output to be close to 1. That means the weights associated with $x_1$ and $x_2$ should be enough to offset $b$ to the point of causing $z$ to be at least 10.\n","* Let's give $b$ a value of -10. How big do we need $w_1$ and $w_2$ to be? \n","    * At least +20\n","* So let's try out $w_1=20$, $w_2=20$, and $b=-10$!\n","\n"]},{"cell_type":"code","metadata":{"id":"mkP-qgVnZFe2","colab_type":"code","colab":{}},"source":["def logic_gate(w1, w2, b):\n","    # Helper to create logic gate functions\n","    # Plug in values for weight_a, weight_b, and bias\n","    return lambda x1, x2: sigmoid(w1 * x1 + w2 * x2 + b)\n","\n","def test(gate):\n","    # Helper function to test out our weight functions.\n","    for a, b in (0, 0), (0, 1), (1, 0), (1, 1):\n","        print(\"{}, {}: {}\".format(a, b, np.round(gate(a, b))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7VNPUJSjZFe5","colab_type":"code","colab":{}},"source":["or_gate = logic_gate(20, 20, -10)\n","test(or_gate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WAy8VKPWZFe-","colab_type":"text"},"source":["<table>\n","\n","<tr>\n","<th colspan=\"3\">OR gate truth table</th>\n","</tr>\n","\n","<tr>\n","<th colspan=\"2\">Input</th>\n","<th>Output</th>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>0</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>1</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>0</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>1</td>\n","<td>1</td>\n","</tr>\n","\n","</table>\n","\n","This matches! Great! Now you try finding the appropriate weight values for each truth table. Try not to guess and check- think through it logically and try to derive values that work.\n","\n","### AND Gate\n","\n","<table>\n","\n","<tr>\n","<th colspan=\"3\">AND gate truth table</th>\n","</tr>\n","\n","<tr>\n","<th colspan=\"2\">Input</th>\n","<th>Output</th>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>0</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>1</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>0</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>1</td>\n","<td>1</td>\n","</tr>\n","\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"W1XyyFyGZFe_","colab_type":"text"},"source":["## Exercise\n","Try to figure out what values for the neurons would make this function as an AND gate."]},{"cell_type":"code","metadata":{"id":"JUIDVVCLZFfA","colab_type":"code","colab":{}},"source":["# TO DO: Fill in the w1, w2, and b parameters such that the truth table matches\n","w1 = \n","w2 = \n","b = \n","and_gate = logic_gate(w1, w2, b)\n","\n","test(and_gate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DqadyhHoZFfG","colab_type":"text"},"source":["## Exercise\n","Do the same for the NOR gate and the NAND gate."]},{"cell_type":"markdown","metadata":{"id":"-wT334v7ZFfH","colab_type":"text"},"source":["### NOR (Not Or) Gate\n","\n","<table>\n","\n","<tr>\n","<th colspan=\"3\">NOR gate truth table</th>\n","</tr>\n","\n","<tr>\n","<th colspan=\"2\">Input</th>\n","<th>Output</th>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>0</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>1</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>0</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>1</td>\n","<td>0</td>\n","</tr>\n","\n","</table>"]},{"cell_type":"code","metadata":{"id":"XS5IHB1-ZFfI","colab_type":"code","outputId":"d9697d11-48c4-4ca1-f6ae-19084c2ac6a7","colab":{}},"source":["# TO DO: Fill in the w1, w2, and b parameters such that the truth table matches\n","w1 = \n","w2 = \n","b = \n","nor_gate = logic_gate(w1, w2, b)\n","\n","test(nor_gate)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0, 0: 1.0\n","0, 1: 0.0\n","1, 0: 0.0\n","1, 1: 0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e8auWjqcZFfN","colab_type":"text"},"source":["### NAND (Not And) Gate\n","\n","<table>\n","\n","<tr>\n","<th colspan=\"3\">NAND gate truth table</th>\n","</tr>\n","\n","<tr>\n","<th colspan=\"2\">Input</th>\n","<th>Output</th>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>0</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>1</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>0</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>1</td>\n","<td>0</td>\n","</tr>\n","\n","</table>"]},{"cell_type":"code","metadata":{"id":"7jJN6VWrZFfO","colab_type":"code","colab":{}},"source":["# TO DO: Fill in the w1, w2, and b parameters such that the truth table matches\n","w1 = \n","w2 = \n","b = \n","nand_gate = logic_gate(w1, w2, b)\n","\n","test(nand_gate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qt--teBRZFfS","colab_type":"text"},"source":["## The limits of single neurons\n","\n","If you've taken computer science courses, you may know that the XOR gates are the basis of computation. They can be used as so-called \"half-adders\", the foundation of being able to add numbers together. Here's the truth table for XOR:\n","\n","### XOR (Exclusive Or) Gate\n","\n","<table>\n","\n","<tr>\n","<th colspan=\"3\">XOR gate truth table</th>\n","</tr>\n","\n","<tr>\n","<th colspan=\"2\">Input</th>\n","<th>Output</th>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>0</td>\n","<td>0</td>\n","</tr>\n","\n","<tr>\n","<td>0</td>\n","<td>1</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>0</td>\n","<td>1</td>\n","</tr>\n","\n","<tr>\n","<td>1</td>\n","<td>1</td>\n","<td>0</td>\n","</tr>\n","\n","</table>\n","\n","Now the question is, can you create a set of weights such that a single neuron can output this property?\n","\n","It turns out that you cannot. Single neurons can't correlate inputs, so it's just confused. So individual neurons are out. Can we still use neurons to somehow form an XOR gate?\n","\n","What if we tried something more complex:\n","\n","![](images/logic03.png)\n","\n","Here, we've got the inputs going to two separate gates: the top neuron is an OR gate, and the bottom is a NAND gate. The output of these gates then get passed to another neuron, which is an AND gate. If you work out the outputs at each combination of input values, you'll see that this is an XOR gate!"]},{"cell_type":"code","metadata":{"id":"wxyd4749ZFfT","colab_type":"code","colab":{}},"source":["# Make sure you have or_gate, nand_gate, and and_gate working from above!\n","def xor_gate(a, b):\n","    c = or_gate(a, b)\n","    d = nand_gate(a, b)\n","    return and_gate(c, d)\n","test(xor_gate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5-Bkr34mZFfY","colab_type":"text"},"source":["## Feedforward Networks as Matrix Computations\n","\n","We discussed previously how the feed-forward computation of a neural network can be thought of as matrix calculations and activation functions.  We will do some actual computations with matrices to see this in action.\n","\n","![](images/FF_NN.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"blaw-XjAZFfZ","colab_type":"text"},"source":["## Exercise\n","Provided below are the following:\n","\n","- Three weight matrices `W_1`, `W_2` and `W_3` representing the weights in each layer.  The convention for these matrices is that each $W_{i,j}$ gives the weight from neuron $i$ in the previous (left) layer to neuron $j$ in the next (right) layer.  \n","- A vector `x_in` representing a single input and a matrix `x_mat_in` representing 7 different inputs.\n","- Two functions: `soft_max_vec` and `soft_max_mat` which apply the soft_max function to a single vector, and row-wise to a matrix.\n","\n","The goals for this exercise are:\n","1. For input `x_in` calculate the inputs and outputs to each layer (assuming sigmoid activations for the middle two layers and soft_max output for the final layer).\n","2. Write a function that does the entire neural network calculation for a single input\n","3. Write a function that does the entire neural network calculation for a matrix of inputs, where each row is a single input.\n","4. Test your functions on `x_in` and `x_mat_in`.\n","\n","This illustrates what happens in a NN during one single forward pass. Roughly speaking, after this forward pass, it remains to compare the output of the network to the known truth values, compute the gradient of the loss function and adjust the weight matrices `W_1`, `W_2` and `W_3` accordingly, and iterate. Hopefully this process will result in better weight matrices and our loss will be smaller afterwards."]},{"cell_type":"code","metadata":{"id":"-nVBRXiOZFfa","colab_type":"code","colab":{}},"source":["W_1 = np.array([[2,-1,1,4],[-1,2,-3,1],[3,-2,-1,5]])\n","W_2 = np.array([[3,1,-2,1],[-2,4,1,-4],[-1,-3,2,-5],[3,1,1,1]])\n","W_3 = np.array([[-1,3,-2],[1,-1,-3],[3,-2,2],[1,2,1]])\n","x_in = np.array([.5,.8,.2])\n","x_mat_in = np.array([[.5,.8,.2],[.1,.9,.6],[.2,.2,.3],[.6,.1,.9],[.5,.5,.4],[.9,.1,.9],[.1,.8,.7]])\n","\n","\n","print('the matrix W_1\\n')\n","print(W_1)\n","print('-'*30)\n","print('vector input x_in\\n')\n","print(x_in)\n","print ('-'*30)\n","print('matrix input x_mat_in -- starts with the vector `x_in`\\n')\n","print(x_mat_in)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_CNHCyeHZFfg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jN1vuf6szIwD","colab_type":"text"},"source":["Follow this image for terminology\n","\n","![1st step of a NN](http://ml.unife.it/wp-content/uploads/2019/09/1st_step_NN.png)\n","\n","- `a_n` is the value in output from a neuron, i.e., neuron input's with activation\n","- `z_n` is the value in input to a neuron, i.e., previous neuron's output * weights"]},{"cell_type":"code","metadata":{"id":"gCcVZuN6ZFfj","colab_type":"code","colab":{}},"source":["## Student to do the calculations below"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIhusUjlZFfm","colab_type":"code","colab":{}},"source":["z_2 = \n","z_2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3cvisfxZFfu","colab_type":"code","colab":{}},"source":["a_2 = \n","a_2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBXq0PeyZFf1","colab_type":"code","colab":{}},"source":["z_3 = \n","z_3"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w6gCHgC0ZFgA","colab_type":"code","colab":{}},"source":["a_3 = \n","a_3"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HjkA21ZwZFgG","colab_type":"code","colab":{}},"source":["z_4 = \n","z_4"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XSiXqaROZFgM","colab_type":"code","colab":{}},"source":["def soft_max_vec(vec):\n","    return # what?\n","\n","def soft_max_mat(mat):\n","    return # what? This is more complicated\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8RnBZCtjZFgP","colab_type":"code","colab":{}},"source":["y_out = soft_max_vec(z_4)\n","y_out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgjF2pFuZFgU","colab_type":"code","colab":{}},"source":["## A one-line function to do the entire neural net computation\n","\n","def nn_comp_vec(x):\n","    return # what?\n","\n","def nn_comp_mat(x):\n","    return # what?"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_0Rj4xavZFgW","colab_type":"code","colab":{}},"source":["nn_comp_vec(x_in)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LTgSmDdRZFga","colab_type":"code","colab":{}},"source":["nn_comp_mat(x_mat_in)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pfbNVwMcZFgd","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}