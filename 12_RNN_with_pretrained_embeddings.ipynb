{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"12_RNN_with_pretrained_embeddings.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-OX6DGc2nk1x","colab_type":"text"},"source":["## Building a RNN to classify text using pre-trained embeddings\n","\n","We will create a very small dataset with 10 sentences, 8 for training and 2 for test, to perfrom classification.\n","\n","We will use pre-trained embeddings. **Note** that these embeddings are trained on general sentences. If you need more specific embeddings you must train them using your examples. In this case we have not enough examples to do so.\n","\n"]},{"cell_type":"code","metadata":{"id":"Z55Bql5evJYN","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","import keras\n","import pandas as pd\n","import re\n","import numpy as np\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Embedding, LSTM, Bidirectional, Flatten\n","from keras.models import Sequential\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ED_53B1vKzA","colab_type":"text"},"source":["## Load the data\n","Usually data is stored in csv files (use of pandas). In this case we manually create sentences to perform a rudimental sentiment analisys."]},{"cell_type":"code","metadata":{"id":"a_Kfy4xXuX2j","colab_type":"code","colab":{}},"source":["traind = {'sentence': [\n","      'Quella pizza era buona',\n","      'Questo gelato fa schifo',\n","      'Il film non mi piace molto',\n","      'La sua nuova canzone non mi piace',\n","      'Adoro il gelato',\n","      'Quella canzone mi piace molto',\n","      'Che brutto!',\n","      'Mi piace!'\n","], 'sentiment': [\n","      'buono',\n","      'cattivo',\n","      'cattivo',\n","      'cattivo',\n","      'buono',\n","      'buono',\n","      'cattivo',\n","      'buono'\n","]}\n","\n","testd = {'sentence': [\n","      'La zuppa non mi piace per niente',\n","      'Questa zuppa sembra ottima'\n","], 'sentiment': [\n","      'cattivo',\n","      'buono'\n","]}\n","\n","train_data = pd.DataFrame(data=traind)\n","test_data = pd.DataFrame(data=testd)\n","\n","print(train_data)\n","print(test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KYuQ8Ctvv4op","colab_type":"text"},"source":["Now we have to clean textual data by removing capital letters and undesired characters such as punctuation."]},{"cell_type":"code","metadata":{"id":"PWp_-RfJwQ5c","colab_type":"code","colab":{}},"source":["train_data['sentence'] = train_data['sentence'].apply(lambda x: x.lower()) # to lowercase\n","train_data['sentence'] = train_data['sentence'].apply((lambda x: re.sub('[^a-z\\s]', '', x))) # remove all characters that are not in a-z\n","\n","test_data['sentence'] = test_data['sentence'].apply(lambda x: x.lower()) # to lowercase\n","test_data['sentence'] = test_data['sentence'].apply((lambda x: re.sub('[^a-z\\s]', '', x))) # remove all characters that are not in a-z\n","\n","print(train_data)\n","print(test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nNu_4XedxaFZ","colab_type":"text"},"source":["The sentences must be splitted in order to get the single words. To do so we can use the Tokenizer.\n","\n","The Tokenizer class of keras allows allows you to tokenize, taking a text and mapping each word into a sequence of integers. Using this class we tokenize the phrases of the dataset."]},{"cell_type":"code","metadata":{"id":"qKttTLsrx7w0","colab_type":"code","colab":{}},"source":["vocab_size = 27 # number of different words, we have 27 different words so we can set anumber greater or equal to 27\n","\n","tokenizer = Tokenizer(num_words=vocab_size, split=' ')\n","tokenizer.fit_on_texts(train_data['sentence'].values)\n","X_train = tokenizer.texts_to_sequences(train_data['sentence'].values)\n","X_train = pad_sequences(X_train)\n","Y_train = pd.get_dummies(train_data['sentiment']).values\n","\n","print('train')\n","print(X_train)\n","print(Y_train)\n","\n","# same for test data\n","tokenizer.fit_on_texts(test_data['sentence'].values)\n","X_test = tokenizer.texts_to_sequences(test_data['sentence'].values)\n","X_test = pad_sequences(X_test)\n","Y_test = pd.get_dummies(test_data['sentiment']).values\n","\n","print('test')\n","print(X_test)\n","print(Y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lfaK7m4W4hNp","colab_type":"code","colab":{}},"source":["embed_dim = 300 # size of Word2Vec embeddings\n","lstm_out = 5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lezBjYYN4tyE","colab_type":"text"},"source":["### Create the model\n","We use two new layers:\n","\n","- **Embedding**: Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]. This layer can only be used as the first layer in a model.\n","\n","- **LSTM**: Implements the Long Short-Term Memory layer.\n","\n","\n"," `Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)`\n","\n","- input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n","- output_dim: int >= 0. Dimension of the dense embedding.\n","- embeddings_initializer: Initializer for the embeddings matrix \n","\n","LSTM has many parameters, some of them are:\n","- units: Positive integer, dimensionality of the output space.\n","- activation: Activation function to use (see activations). Default: hyperbolic tangent (tanh). If you pass None, no activation is applied (ie. \"linear\" activation: a(x) = x).\n","\n"]},{"cell_type":"code","metadata":{"id":"a4rbCb_34xCF","colab_type":"code","colab":{}},"source":["model = Sequential()\n","model.add(Embedding(vocab_size, embed_dim, input_length=X_train.shape[1]))\n","model.add(LSTM(lstm_out, activation='tanh', dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(2, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMyQKEII5FuF","colab_type":"code","colab":{}},"source":["history = model.fit(X_train, Y_train, epochs=50, \n","                    batch_size=100, verbose=2, shuffle=True)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zGpz2HH38CLU","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots()\n","ax.plot(history.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n","ax.plot(history.history[\"acc\"],'g', marker='.', label=\"Train acc\")\n","ax.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zpHRRO7F8fDS","colab_type":"code","colab":{}},"source":["model.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IOkfA4e181bD","colab_type":"code","colab":{}},"source":["model.evaluate(X_test, Y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y__pQdSEBC5D","colab_type":"text"},"source":["Let's try another network.\n","\n","We try to use a Bidirectional LSTM, were the state is passed both forward and backward.\n","\n","**Bidirectional** is a bidirectional wrapper for RNNs."]},{"cell_type":"code","metadata":{"id":"CDyDd4cF-UG_","colab_type":"code","colab":{}},"source":["model1 = Sequential()\n","model1.add(Embedding(vocab_size, embed_dim, input_length=X_train.shape[1]))\n","model1.add(Bidirectional(LSTM(lstm_out, activation='tanh', dropout=0.2, recurrent_dropout=0.2)))\n","model1.add(Dense(2, activation='softmax'))\n","model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model1.summary()\n","\n","history = model1.fit(X_train, Y_train, epochs=50, \n","                    batch_size=100, verbose=2, shuffle=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZgrOsjf-_Zm","colab_type":"code","colab":{}},"source":["model1.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5fflYwoD_Bxd","colab_type":"code","colab":{}},"source":["model1.evaluate(X_test, Y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HlvAmagsBx5q","colab_type":"text"},"source":["We can also add more LSTM layers.\n","\n","To do so we must fix the parameter `return_sequences` to True in all the LSTM except the last."]},{"cell_type":"code","metadata":{"id":"qOAGZK1w-sks","colab_type":"code","colab":{}},"source":["model2 = Sequential()\n","model2.add(Embedding(vocab_size, embed_dim, input_length=X_train.shape[1]))\n","model2.add(Bidirectional(LSTM(lstm_out, return_sequences=True, # this option must be used if we have more LSTM\n","                             activation='tanh', dropout=0.2, recurrent_dropout=0.2)))\n","model2.add(LSTM(lstm_out, return_sequences=True))\n","model2.add(LSTM(lstm_out))\n","model2.add(Dense(2, activation='softmax'))\n","model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model2.summary()\n","\n","history = model2.fit(X_train, Y_train, epochs=50, \n","                    batch_size=100, verbose=2, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lTnqImhJ_FPX","colab_type":"code","colab":{}},"source":["model2.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vo5zMJzu_GNf","colab_type":"code","colab":{}},"source":["model2.evaluate(X_test, Y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dn0rOgg5CJGm","colab_type":"text"},"source":["### Use of pretrained Embeddings\n","As seen, the performances are really poor due to the very small size of the dataset.\n","\n","We can try to use pre-trained embeddings (probably with this dataset we won't get good results as well).\n","\n","To do so we must download the pre-trained model, there are model for Word2Vec, GloVe, and many others. For each model there may be also many different versions tailored on different languages.\n","\n","In our case, we should download the model for the Italian language. Usualy such models are around 1-2 GBs.\n","\n","Once downloaded we must load them:"]},{"cell_type":"code","metadata":{"id":"3eQdLIpy0jSG","colab_type":"code","colab":{}},"source":["from gensim.models import Word2Vec\n","\n","# Here we load a pretrained model and keep only the embeddings we need\n","# GloVe downloadable here: https://drive.google.com/file/d/1ZODMv0guq8OgZN0fq_V3J3s5ZqsjW99U/view?usp=sharing\n","# m = Word2Vec.load('./glove_WIKI')\n","# Word2Vec downloadable here: https://drive.google.com/file/d/17LPOi8aVISuwq4g5hn2Ddj-cQYrbXlgZ/view?usp=sharing\n","# m = Word2Vec.load(\n","#    './wiki_iter=5_algorithm=skipgram_window=10_size=300_neg-samples=10.m')\n","embedding_matrix = zeros((vocab_size, embed_dim))\n","\n","for word, i in tokenizer.word_index.items():\n","  if word in m.wv:\n","    print(word)\n","    embedding_matrix[i] = m.wv[word] # we take here only the embedding of the words we have in our vocabulary"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d9e4EQNlDAY1","colab_type":"text"},"source":["Now we can build our model."]},{"cell_type":"code","metadata":{"id":"y2S2f4fQDU2t","colab_type":"code","colab":{}},"source":["model2 = Sequential()\n","model.add(Embedding(vocab_size, embed_dim, weights=[embedding_matrix],\n","                    input_length=X.shape[1], trainable=False))\n","model2.add(Bidirectional(LSTM(lstm_out, return_sequences=True, # this option must be used if we have more LSTM\n","                             activation='tanh', dropout=0.2, recurrent_dropout=0.2)))\n","model2.add(LSTM(lstm_out, return_sequences=True))\n","model2.add(LSTM(lstm_out))\n","model2.add(Dense(2, activation='softmax'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PrUuPz2vDZXx","colab_type":"text"},"source":["What changes her is the `Embedding` layer, where we initialize the weights withthose of the pre-trained model (`weights=[embedding_matrix]`).\n","Moreover, we can decide whether to tune these weigths during the training or keep them fixed by using the parameter `trainable`."]}]}