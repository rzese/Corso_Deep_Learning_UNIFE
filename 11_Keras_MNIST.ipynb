{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"tensorflow","language":"python","name":"tensorflow"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"11_Keras_MNIST.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"_ZE5uf94sLr7","colab_type":"text"},"source":["## Handwritten Image Detection with Keras using MNIST data\n","\n","In this exercise we will work with image data: specifically the famous MNIST data set.  This data set contains 70,000 images of handwritten digits in grayscale (0=black, 255 = white), 60,000 for training and 10,000 for testing.  The images are 28 pixels by 28 pixels for a total of 784 pixels.  This is quite small by image standards.  Also, the images are well centered and isolated.  This makes this problem solvable with standard fully connected neural nets without too much pre-work."]},{"cell_type":"markdown","metadata":{"id":"nzlm8TewsLr9","colab_type":"text"},"source":["In the first part of this notebook, we will walk you through loading in the data, building a fully connected network, and training it.  Then it will be your turn to implement LeNet-5, try other different models and see if you can improve performance"]},{"cell_type":"code","metadata":{"id":"xBc8G6T3sLr-","colab_type":"code","colab":{}},"source":["# Preliminaries\n","\n","from __future__ import print_function\n","\n","import keras\n","from keras.datasets import mnist # easy loading of MNIST\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from keras.optimizers import RMSprop\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"wqU5JJUQsLsC","colab_type":"text"},"source":["Let's explore the dataset a little bit"]},{"cell_type":"code","metadata":{"id":"yAwg6lkVsLsD","colab_type":"code","colab":{}},"source":["# Load the data, shuffled and split between train and test sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cShCyX_9sLsG","colab_type":"code","colab":{}},"source":["x_train[0].shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IQS2u3h9sLsO","colab_type":"code","colab":{}},"source":["#Let's just look at a particular example to see what is inside\n","\n","x_train[333]  ## Just a 28 x 28 numpy array of ints from 0 to 255"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4RevbjWHsLsT","colab_type":"code","colab":{}},"source":["# What is the corresponding label in the training set?\n","y_train[333]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jtw7TcbdsLsX","colab_type":"code","colab":{}},"source":["# Let's see what this image actually looks like\n","\n","plt.imshow(x_train[333], cmap='Greys_r')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yXZPkKrZsLse","colab_type":"code","colab":{}},"source":["# this is the shape of the np.array x_train\n","# it is 3 dimensional.\n","print(x_train.shape, 'train samples')\n","print(x_test.shape, 'test samples')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nca4dr_5sLsj","colab_type":"code","colab":{}},"source":["## For our purposes, these images are just a vector of 784 inputs, so let's convert\n","fc_x_train = x_train.reshape(len(x_train), 28*28)\n","fc_x_test = x_test.reshape(len(x_test), 28*28)\n","\n","## Keras works with floats, so we must cast the numbers to floats\n","fc_x_train = fc_x_train.astype('float32')\n","fc_x_test = fc_x_test.astype('float32')\n","\n","## Normalize the inputs so they are between 0 and 1\n","fc_x_train /= 255\n","fc_x_test /= 255\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ERx7gE_sLsm","colab_type":"code","colab":{}},"source":["# convert class vectors to binary class matrices\n","num_classes = 10\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","y_train[333]  # now the digit k is represented by a 1 in the kth entry (0-indexed) of the length 10 vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZOcyK8UqsLsr","colab_type":"code","colab":{}},"source":["# We will build a model with two hidden layers of size 512\n","# Fully connected inputs at each layer\n","# We will use dropout of .2 to help regularize\n","model_1 = Sequential()\n","model_1.add(Dense(64, activation='relu', input_shape=(784,)))\n","model_1.add(Dropout(0.2))\n","model_1.add(Dense(64, activation='relu'))\n","model_1.add(Dropout(0.2))\n","model_1.add(Dense(10, activation='softmax'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLHPplBHsLsv","colab_type":"code","colab":{}},"source":["## Note that this model has a LOT of parameters\n","model_1.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTL3ANg6sLsz","colab_type":"code","colab":{}},"source":["# Let's compile the model\n","learning_rate = .001\n","model_1.compile(loss='categorical_crossentropy',\n","              optimizer=RMSprop(lr=learning_rate),\n","              metrics=['accuracy'])\n","# note that `categorical cross entropy` is the natural generalization \n","# of the loss function we had in binary classification case, to multi class case"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l-xx7KQ_sLs3","colab_type":"code","colab":{}},"source":["# And now let's fit.\n","\n","batch_size = 128  # mini-batch with 128 examples\n","epochs = 30\n","history_1 = model_1.fit(\n","    fc_x_train, y_train,\n","    batch_size=batch_size,\n","    epochs=epochs,\n","    verbose=1,\n","    validation_data=(fc_x_test, y_test)) # this for plotting the loss curve"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0mHUQ0ggsLs9","colab_type":"code","colab":{}},"source":["## We will use Keras evaluate function to evaluate performance on the test set\n","\n","score_1 = model_1.evaluate(fc_x_test, y_test, verbose=0)\n","print('Test loss:', score_1[0])\n","print('Test accuracy:', score_1[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMN2yq5SsLtC","colab_type":"code","colab":{}},"source":["def plot_loss_accuracy(history):\n","    fig = plt.figure(figsize=(12, 6))\n","    ax = fig.add_subplot(1, 2, 1)\n","    ax.plot(history.history[\"loss\"],'r-x', label=\"Train Loss\")\n","    ax.plot(history.history[\"val_loss\"],'b-x', label=\"Validation Loss\")\n","    ax.legend()\n","    ax.set_title('cross_entropy loss')\n","    ax.grid(True)\n","\n","\n","    ax = fig.add_subplot(1, 2, 2)\n","    ax.plot(history.history[\"acc\"],'r-x', label=\"Train Accuracy\")\n","    ax.plot(history.history[\"val_acc\"],'b-x', label=\"Validation Accuracy\")\n","    ax.legend()\n","    ax.set_title('accuracy')\n","    ax.grid(True)\n","    \n","\n","plot_loss_accuracy(history_1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EelVfhh8sLtM","colab_type":"text"},"source":["This is reasonably good performance, but we can do even better!  Next you will build an even bigger network and compare the performance."]},{"cell_type":"markdown","metadata":{"id":"-4AFzbVosLtN","colab_type":"text"},"source":["## Exercise\n","### Your Turn: Build your own model\n","Use the Keras \"Sequential\" functionality to build `model_2` with the following specifications:\n","\n","1. Two hidden layers.\n","2. First hidden layer of size 400 and second of size 300\n","3. Dropout of .4 at each layer\n","4. How many parameters does your model have?  How does it compare with the previous model?\n","4. Train this model for 20 epochs with RMSProp at a learning rate of .001 and a batch size of 128\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"V3DPbaD4sLtO","colab_type":"code","colab":{}},"source":["### Build your model here\n","\n","\n","### Print its summary\n","\n","batch_size = \n","epochs = \n","learning_rate = \n","\n","\n","### Compile it. Remember to set optimizer and learning rate\n","\n","\n","### Fit and Evaluate the model\n","print('')\n","print('Test loss:', ? )\n","print('Test accuracy:', ?)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eaNIAuIsypw9","colab_type":"code","colab":{}},"source":["plot_loss_accuracy(history_2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p_qtHM5aw_If","colab_type":"text"},"source":["### Your Turn: Build your own model\n","Implement LeNet-5 in `model_3`.\n","\n","How many parameters does your model have?  How does it compare with the previous model?"]},{"cell_type":"code","metadata":{"id":"vQtMNT4JxPx0","colab_type":"code","colab":{}},"source":["### Build your model here\n","from keras.layers import Conv2D, AveragePooling2D, Flatten\n","\n","\n","### Print its summary\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-94Erd_NyGCA","colab_type":"code","colab":{}},"source":["## prepare the images\n","# Conv2D accept input of the form (batch, rows, cols, channels) -> we need to add a new column to x_train and x_test\n","# See https://keras.io/layers/convolutional/ for more details\n","# Reshape the training and test set\n","ln_x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) # <- the reshape is necessary as Conv2D layer takes as input examples that are a matrix with a cell for each pixel and each pixel is an array \n","ln_x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)    #    containing information about each channel of the image. So, wih length 1 for greyscale, 3 for RGB, 4 for CMYK, etc.\n","\n","# Padding the images by 2 pixels since in the paper input images were 32x32\n","ln_x_train = np.pad(ln_x_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n","#                             padding at the beginning and end of the first dimension -> 60000\n","#                                   padding for the second dimension (we are here in the matrix) -> 2 pixels at the beginning and at the end of each row\n","#                                          padding for the second dimension (we are here in the matrix) -> 2 pixels at the beginning and at the end of each column\n","#                                                   padding for the number of channel\n","ln_x_test = np.pad(ln_x_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n","\n","## Keras works with floats, so we must cast the numbers to floats\n","ln_x_train = ln_x_train.astype('float32')\n","ln_x_test = ln_x_test.astype('float32')\n","\n","## Normalize the inputs so they are between 0 and 1\n","ln_x_train /= 255\n","ln_x_test /= 255\n","\n","# this is the shape of the np.array ln_x_train\n","# it is 3 dimensional.\n","print(ln_x_train.shape, 'train samples')\n","print(ln_x_test.shape, 'test samples')\n","\n","#Visualizing the data\n","plt.imshow(ln_x_train[333,:,:,0], cmap='Greys_r')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IPF1nwsV2MD_","colab_type":"code","colab":{}},"source":["model_3.compile(loss='categorical_crossentropy',\n","              optimizer='adam',  # use of Adam here\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"86-OlG2s52gL","colab_type":"code","colab":{}},"source":["batch_size = 128\n","epochs = 10\n","\n","\n","history_3 = model_3.fit(ln_x_train, y_train,\n","                    batch_size=batch_size,\n","                    epochs=epochs,\n","                    verbose=1,\n","                    validation_data=(ln_x_test, y_test))\n","\n","score_3 = model_3.evaluate(ln_x_test, y_test, verbose=0)\n","print('')\n","print('Test loss:', score_3[0])\n","print('Test accuracy:', score_3[1])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2iRg5H0gDwk8","colab_type":"code","colab":{}},"source":["plot_loss_accuracy(history_3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Douf1cHvsLtS","colab_type":"text"},"source":["## SOLUTION"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"yG8xUSCBsLtU","colab_type":"text"},"source":["## Think about the following questions\n","\n","1) How do `model_1`, `model_2` and `model_3` compare?  Which do you prefer?  If you were going to put one into production, which would you choose and why?\n","\n","2) Compare the trajectories of the loss function on the training set and test set for each model?  How do they compare?  What does that suggest about each model?  Do the same for accuracy?  Which do you think is more meaningful, the loss or the accuracy?\n","\n","3) Suggest an improvement to one of the models (changing structure, learning rate, number of epochs, etc.) that you think will result in a better model.  Try it out below?  Did it improve the performance?"]},{"cell_type":"code","metadata":{"id":"Wi5A7tLisLtV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}