{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "\n",
    "from __future__ import absolute_import, division, print_function  # Python 2/3 compatibility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Import Keras objects for Deep Learning\n",
    "\n",
    "from keras.models  import Sequential, K\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load in the data set (Internet Access needed)\n",
    "\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = [\"times_pregnant\", \"glucose_tolerance_test\", \"blood_pressure\", \"skin_thickness\", \"insulin\", \n",
    "         \"bmi\", \"pedigree_function\", \"age\", \"has_diabetes\"]\n",
    "diabetes_df = pd.read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times_pregnant</th>\n",
       "      <th>glucose_tolerance_test</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree_function</th>\n",
       "      <th>age</th>\n",
       "      <th>has_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>9</td>\n",
       "      <td>171</td>\n",
       "      <td>110</td>\n",
       "      <td>24</td>\n",
       "      <td>240</td>\n",
       "      <td>45.4</td>\n",
       "      <td>0.721</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>2</td>\n",
       "      <td>115</td>\n",
       "      <td>64</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>30.8</td>\n",
       "      <td>0.421</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>58</td>\n",
       "      <td>35</td>\n",
       "      <td>90</td>\n",
       "      <td>21.8</td>\n",
       "      <td>0.155</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>8</td>\n",
       "      <td>133</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.270</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     times_pregnant  glucose_tolerance_test  blood_pressure  skin_thickness  \\\n",
       "81                2                      74               0               0   \n",
       "43                9                     171             110              24   \n",
       "423               2                     115              64              22   \n",
       "656               2                     101              58              35   \n",
       "61                8                     133              72               0   \n",
       "\n",
       "     insulin   bmi  pedigree_function  age  has_diabetes  \n",
       "81         0   0.0              0.102   22             0  \n",
       "43       240  45.4              0.721   54             1  \n",
       "423        0  30.8              0.421   21             0  \n",
       "656       90  21.8              0.155   22             0  \n",
       "61         0  32.9              0.270   39             1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at the data -- if there are lots of \"NaN\" you may have internet connectivity issues\n",
    "print(diabetes_df.shape)\n",
    "diabetes_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   6.   ,  148.   ,   72.   , ...,   33.6  ,    0.627,   50.   ],\n",
       "        [   1.   ,   85.   ,   66.   , ...,   26.6  ,    0.351,   31.   ],\n",
       "        [   8.   ,  183.   ,   64.   , ...,   23.3  ,    0.672,   32.   ],\n",
       "        ..., \n",
       "        [   5.   ,  121.   ,   72.   , ...,   26.2  ,    0.245,   30.   ],\n",
       "        [   1.   ,  126.   ,   60.   , ...,   30.1  ,    0.349,   47.   ],\n",
       "        [   1.   ,   93.   ,   70.   , ...,   30.4  ,    0.315,   23.   ]]),\n",
       " array([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 1, 0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values\n",
    "y = diabetes_df[\"has_diabetes\"].values\n",
    "(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((576, 8), (192, 8))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data to Train, and Test (75%, 25%)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)\n",
    "(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34895833333333331, 0.65104166666666663)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n",
    "## Exercise: Get a baseline performance using Random Forest\n",
    "To begin, and get a baseline for classifier performance:\n",
    "1. Train a Random Forest model with 200 trees on the training data.\n",
    "2. Calculate the accuracy and roc_auc_score of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.776\n",
      "roc-auc is 0.835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 1, 1, 0]), array([[ 0.59 ,  0.41 ],\n",
       "        [ 0.375,  0.625],\n",
       "        [ 0.44 ,  0.56 ],\n",
       "        [ 0.88 ,  0.12 ],\n",
       "        [ 0.87 ,  0.13 ],\n",
       "        [ 0.565,  0.435],\n",
       "        [ 0.955,  0.045],\n",
       "        [ 0.61 ,  0.39 ],\n",
       "        [ 0.08 ,  0.92 ],\n",
       "        [ 0.63 ,  0.37 ],\n",
       "        [ 0.91 ,  0.09 ],\n",
       "        [ 0.79 ,  0.21 ],\n",
       "        [ 0.31 ,  0.69 ],\n",
       "        [ 0.96 ,  0.04 ],\n",
       "        [ 0.115,  0.885],\n",
       "        [ 0.335,  0.665],\n",
       "        [ 0.475,  0.525],\n",
       "        [ 0.83 ,  0.17 ],\n",
       "        [ 0.39 ,  0.61 ],\n",
       "        [ 0.875,  0.125],\n",
       "        [ 0.64 ,  0.36 ],\n",
       "        [ 0.995,  0.005],\n",
       "        [ 0.87 ,  0.13 ],\n",
       "        [ 0.21 ,  0.79 ],\n",
       "        [ 0.38 ,  0.62 ],\n",
       "        [ 0.245,  0.755],\n",
       "        [ 0.405,  0.595],\n",
       "        [ 0.195,  0.805],\n",
       "        [ 0.98 ,  0.02 ],\n",
       "        [ 0.115,  0.885],\n",
       "        [ 0.845,  0.155],\n",
       "        [ 0.57 ,  0.43 ],\n",
       "        [ 0.83 ,  0.17 ],\n",
       "        [ 0.59 ,  0.41 ],\n",
       "        [ 0.72 ,  0.28 ],\n",
       "        [ 0.185,  0.815],\n",
       "        [ 0.21 ,  0.79 ],\n",
       "        [ 0.51 ,  0.49 ],\n",
       "        [ 0.885,  0.115],\n",
       "        [ 0.725,  0.275],\n",
       "        [ 0.215,  0.785],\n",
       "        [ 0.59 ,  0.41 ],\n",
       "        [ 0.495,  0.505],\n",
       "        [ 0.33 ,  0.67 ],\n",
       "        [ 0.895,  0.105],\n",
       "        [ 0.88 ,  0.12 ],\n",
       "        [ 0.47 ,  0.53 ],\n",
       "        [ 0.745,  0.255],\n",
       "        [ 1.   ,  0.   ],\n",
       "        [ 0.215,  0.785],\n",
       "        [ 0.455,  0.545],\n",
       "        [ 0.405,  0.595],\n",
       "        [ 0.275,  0.725],\n",
       "        [ 0.745,  0.255],\n",
       "        [ 0.485,  0.515],\n",
       "        [ 0.88 ,  0.12 ],\n",
       "        [ 0.84 ,  0.16 ],\n",
       "        [ 0.76 ,  0.24 ],\n",
       "        [ 0.35 ,  0.65 ],\n",
       "        [ 0.97 ,  0.03 ],\n",
       "        [ 0.96 ,  0.04 ],\n",
       "        [ 0.72 ,  0.28 ],\n",
       "        [ 0.92 ,  0.08 ],\n",
       "        [ 1.   ,  0.   ],\n",
       "        [ 0.985,  0.015],\n",
       "        [ 0.28 ,  0.72 ],\n",
       "        [ 0.995,  0.005],\n",
       "        [ 0.995,  0.005],\n",
       "        [ 0.9  ,  0.1  ],\n",
       "        [ 0.71 ,  0.29 ],\n",
       "        [ 0.405,  0.595],\n",
       "        [ 0.97 ,  0.03 ],\n",
       "        [ 0.925,  0.075],\n",
       "        [ 0.89 ,  0.11 ],\n",
       "        [ 0.59 ,  0.41 ],\n",
       "        [ 0.63 ,  0.37 ],\n",
       "        [ 0.73 ,  0.27 ],\n",
       "        [ 0.375,  0.625],\n",
       "        [ 0.605,  0.395],\n",
       "        [ 0.66 ,  0.34 ],\n",
       "        [ 0.185,  0.815],\n",
       "        [ 0.985,  0.015],\n",
       "        [ 0.985,  0.015],\n",
       "        [ 0.91 ,  0.09 ],\n",
       "        [ 0.53 ,  0.47 ],\n",
       "        [ 0.985,  0.015],\n",
       "        [ 0.73 ,  0.27 ],\n",
       "        [ 0.39 ,  0.61 ],\n",
       "        [ 0.28 ,  0.72 ],\n",
       "        [ 0.515,  0.485],\n",
       "        [ 0.965,  0.035],\n",
       "        [ 0.97 ,  0.03 ],\n",
       "        [ 0.37 ,  0.63 ],\n",
       "        [ 0.345,  0.655],\n",
       "        [ 0.25 ,  0.75 ],\n",
       "        [ 0.87 ,  0.13 ],\n",
       "        [ 0.4  ,  0.6  ],\n",
       "        [ 0.65 ,  0.35 ],\n",
       "        [ 0.91 ,  0.09 ],\n",
       "        [ 0.56 ,  0.44 ],\n",
       "        [ 0.43 ,  0.57 ],\n",
       "        [ 0.99 ,  0.01 ],\n",
       "        [ 0.79 ,  0.21 ],\n",
       "        [ 0.39 ,  0.61 ],\n",
       "        [ 0.645,  0.355],\n",
       "        [ 0.615,  0.385],\n",
       "        [ 0.94 ,  0.06 ],\n",
       "        [ 0.955,  0.045],\n",
       "        [ 0.83 ,  0.17 ],\n",
       "        [ 0.21 ,  0.79 ],\n",
       "        [ 0.605,  0.395],\n",
       "        [ 0.725,  0.275],\n",
       "        [ 0.63 ,  0.37 ],\n",
       "        [ 0.82 ,  0.18 ],\n",
       "        [ 0.815,  0.185],\n",
       "        [ 0.99 ,  0.01 ],\n",
       "        [ 0.85 ,  0.15 ],\n",
       "        [ 0.795,  0.205],\n",
       "        [ 0.72 ,  0.28 ],\n",
       "        [ 0.95 ,  0.05 ],\n",
       "        [ 0.36 ,  0.64 ],\n",
       "        [ 0.86 ,  0.14 ],\n",
       "        [ 0.41 ,  0.59 ],\n",
       "        [ 0.905,  0.095],\n",
       "        [ 0.4  ,  0.6  ],\n",
       "        [ 0.285,  0.715],\n",
       "        [ 0.61 ,  0.39 ],\n",
       "        [ 0.435,  0.565],\n",
       "        [ 0.63 ,  0.37 ],\n",
       "        [ 0.9  ,  0.1  ],\n",
       "        [ 0.35 ,  0.65 ],\n",
       "        [ 0.975,  0.025],\n",
       "        [ 0.67 ,  0.33 ],\n",
       "        [ 0.79 ,  0.21 ],\n",
       "        [ 0.92 ,  0.08 ],\n",
       "        [ 0.845,  0.155],\n",
       "        [ 0.885,  0.115],\n",
       "        [ 0.855,  0.145],\n",
       "        [ 1.   ,  0.   ],\n",
       "        [ 0.96 ,  0.04 ],\n",
       "        [ 0.585,  0.415],\n",
       "        [ 0.225,  0.775],\n",
       "        [ 0.325,  0.675],\n",
       "        [ 0.71 ,  0.29 ],\n",
       "        [ 0.74 ,  0.26 ],\n",
       "        [ 0.805,  0.195],\n",
       "        [ 0.8  ,  0.2  ],\n",
       "        [ 0.915,  0.085],\n",
       "        [ 0.37 ,  0.63 ],\n",
       "        [ 0.48 ,  0.52 ],\n",
       "        [ 0.71 ,  0.29 ],\n",
       "        [ 0.765,  0.235],\n",
       "        [ 0.685,  0.315],\n",
       "        [ 0.465,  0.535],\n",
       "        [ 0.885,  0.115],\n",
       "        [ 0.735,  0.265],\n",
       "        [ 0.3  ,  0.7  ],\n",
       "        [ 0.63 ,  0.37 ],\n",
       "        [ 0.31 ,  0.69 ],\n",
       "        [ 0.55 ,  0.45 ],\n",
       "        [ 0.495,  0.505],\n",
       "        [ 0.705,  0.295],\n",
       "        [ 0.93 ,  0.07 ],\n",
       "        [ 0.61 ,  0.39 ],\n",
       "        [ 0.745,  0.255],\n",
       "        [ 0.285,  0.715],\n",
       "        [ 0.74 ,  0.26 ],\n",
       "        [ 0.685,  0.315],\n",
       "        [ 0.835,  0.165],\n",
       "        [ 0.78 ,  0.22 ],\n",
       "        [ 0.53 ,  0.47 ],\n",
       "        [ 0.42 ,  0.58 ],\n",
       "        [ 0.475,  0.525],\n",
       "        [ 0.995,  0.005],\n",
       "        [ 0.185,  0.815],\n",
       "        [ 0.98 ,  0.02 ],\n",
       "        [ 0.995,  0.005],\n",
       "        [ 0.835,  0.165],\n",
       "        [ 0.56 ,  0.44 ],\n",
       "        [ 0.795,  0.205],\n",
       "        [ 0.4  ,  0.6  ],\n",
       "        [ 0.985,  0.015],\n",
       "        [ 0.75 ,  0.25 ],\n",
       "        [ 0.885,  0.115],\n",
       "        [ 0.475,  0.525],\n",
       "        [ 0.55 ,  0.45 ],\n",
       "        [ 1.   ,  0.   ],\n",
       "        [ 0.625,  0.375],\n",
       "        [ 0.415,  0.585],\n",
       "        [ 0.295,  0.705],\n",
       "        [ 0.39 ,  0.61 ],\n",
       "        [ 0.82 ,  0.18 ]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))\n",
    "(y_pred_class_rf,y_pred_prob_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzOzJuPmbGZm1mT+BTnJ9zJI3LvxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1b6ec390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Single Hidden Layer Neural Network\n",
    "\n",
    "We will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## First let's normalize the data\n",
    "## This aids the training of neural nets by providing numerical stability\n",
    "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
    "\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the Model \n",
    "# Input size is 8-dimensional\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(12, input_shape=(8,), activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  This is a nice tool to view the model you have created and count the parameters\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension question:\n",
    "Why do we have 121 parameters?  Does that make sense?\n",
    "\n",
    "\n",
    "Let's fit our model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "576/576 [==============================] - 0s - loss: 0.7154 - acc: 0.6181 - val_loss: 0.7046 - val_acc: 0.5990\n",
      "Epoch 2/200\n",
      "576/576 [==============================] - 0s - loss: 0.7068 - acc: 0.6198 - val_loss: 0.6973 - val_acc: 0.6042\n",
      "Epoch 3/200\n",
      "576/576 [==============================] - 0s - loss: 0.6986 - acc: 0.6285 - val_loss: 0.6904 - val_acc: 0.6094\n",
      "Epoch 4/200\n",
      "576/576 [==============================] - 0s - loss: 0.6906 - acc: 0.6354 - val_loss: 0.6838 - val_acc: 0.6094\n",
      "Epoch 5/200\n",
      "576/576 [==============================] - 0s - loss: 0.6830 - acc: 0.6424 - val_loss: 0.6776 - val_acc: 0.6146\n",
      "Epoch 6/200\n",
      "576/576 [==============================] - 0s - loss: 0.6756 - acc: 0.6476 - val_loss: 0.6716 - val_acc: 0.6198\n",
      "Epoch 7/200\n",
      "576/576 [==============================] - 0s - loss: 0.6686 - acc: 0.6476 - val_loss: 0.6659 - val_acc: 0.6250\n",
      "Epoch 8/200\n",
      "576/576 [==============================] - 0s - loss: 0.6619 - acc: 0.6476 - val_loss: 0.6605 - val_acc: 0.6250\n",
      "Epoch 9/200\n",
      "576/576 [==============================] - 0s - loss: 0.6555 - acc: 0.6476 - val_loss: 0.6553 - val_acc: 0.6302\n",
      "Epoch 10/200\n",
      "576/576 [==============================] - 0s - loss: 0.6494 - acc: 0.6528 - val_loss: 0.6503 - val_acc: 0.6406\n",
      "Epoch 11/200\n",
      "576/576 [==============================] - 0s - loss: 0.6435 - acc: 0.6580 - val_loss: 0.6455 - val_acc: 0.6406\n",
      "Epoch 12/200\n",
      "576/576 [==============================] - 0s - loss: 0.6378 - acc: 0.6615 - val_loss: 0.6409 - val_acc: 0.6458\n",
      "Epoch 13/200\n",
      "576/576 [==============================] - 0s - loss: 0.6323 - acc: 0.6632 - val_loss: 0.6365 - val_acc: 0.6458\n",
      "Epoch 14/200\n",
      "576/576 [==============================] - 0s - loss: 0.6271 - acc: 0.6649 - val_loss: 0.6323 - val_acc: 0.6458\n",
      "Epoch 15/200\n",
      "576/576 [==============================] - 0s - loss: 0.6220 - acc: 0.6736 - val_loss: 0.6282 - val_acc: 0.6458\n",
      "Epoch 16/200\n",
      "576/576 [==============================] - 0s - loss: 0.6171 - acc: 0.6823 - val_loss: 0.6243 - val_acc: 0.6510\n",
      "Epoch 17/200\n",
      "576/576 [==============================] - 0s - loss: 0.6123 - acc: 0.6823 - val_loss: 0.6205 - val_acc: 0.6562\n",
      "Epoch 18/200\n",
      "576/576 [==============================] - 0s - loss: 0.6078 - acc: 0.6858 - val_loss: 0.6169 - val_acc: 0.6615\n",
      "Epoch 19/200\n",
      "576/576 [==============================] - 0s - loss: 0.6034 - acc: 0.6892 - val_loss: 0.6134 - val_acc: 0.6719\n",
      "Epoch 20/200\n",
      "576/576 [==============================] - 0s - loss: 0.5992 - acc: 0.6927 - val_loss: 0.6100 - val_acc: 0.6771\n",
      "Epoch 21/200\n",
      "576/576 [==============================] - 0s - loss: 0.5951 - acc: 0.6927 - val_loss: 0.6067 - val_acc: 0.6823\n",
      "Epoch 22/200\n",
      "576/576 [==============================] - 0s - loss: 0.5911 - acc: 0.6944 - val_loss: 0.6036 - val_acc: 0.6875\n",
      "Epoch 23/200\n",
      "576/576 [==============================] - 0s - loss: 0.5873 - acc: 0.7066 - val_loss: 0.6006 - val_acc: 0.6979\n",
      "Epoch 24/200\n",
      "576/576 [==============================] - 0s - loss: 0.5835 - acc: 0.7118 - val_loss: 0.5977 - val_acc: 0.7031\n",
      "Epoch 25/200\n",
      "576/576 [==============================] - 0s - loss: 0.5800 - acc: 0.7188 - val_loss: 0.5949 - val_acc: 0.7031\n",
      "Epoch 26/200\n",
      "576/576 [==============================] - 0s - loss: 0.5766 - acc: 0.7222 - val_loss: 0.5922 - val_acc: 0.7135\n",
      "Epoch 27/200\n",
      "576/576 [==============================] - 0s - loss: 0.5733 - acc: 0.7257 - val_loss: 0.5896 - val_acc: 0.7135\n",
      "Epoch 28/200\n",
      "576/576 [==============================] - 0s - loss: 0.5701 - acc: 0.7309 - val_loss: 0.5872 - val_acc: 0.7135\n",
      "Epoch 29/200\n",
      "576/576 [==============================] - 0s - loss: 0.5671 - acc: 0.7344 - val_loss: 0.5848 - val_acc: 0.7135\n",
      "Epoch 30/200\n",
      "576/576 [==============================] - 0s - loss: 0.5642 - acc: 0.7361 - val_loss: 0.5824 - val_acc: 0.7135\n",
      "Epoch 31/200\n",
      "576/576 [==============================] - 0s - loss: 0.5613 - acc: 0.7378 - val_loss: 0.5802 - val_acc: 0.7135\n",
      "Epoch 32/200\n",
      "576/576 [==============================] - 0s - loss: 0.5586 - acc: 0.7396 - val_loss: 0.5780 - val_acc: 0.7135\n",
      "Epoch 33/200\n",
      "576/576 [==============================] - 0s - loss: 0.5559 - acc: 0.7378 - val_loss: 0.5758 - val_acc: 0.7135\n",
      "Epoch 34/200\n",
      "576/576 [==============================] - 0s - loss: 0.5533 - acc: 0.7378 - val_loss: 0.5737 - val_acc: 0.7188\n",
      "Epoch 35/200\n",
      "576/576 [==============================] - 0s - loss: 0.5507 - acc: 0.7361 - val_loss: 0.5717 - val_acc: 0.7188\n",
      "Epoch 36/200\n",
      "576/576 [==============================] - 0s - loss: 0.5483 - acc: 0.7344 - val_loss: 0.5698 - val_acc: 0.7240\n",
      "Epoch 37/200\n",
      "576/576 [==============================] - 0s - loss: 0.5459 - acc: 0.7344 - val_loss: 0.5679 - val_acc: 0.7396\n",
      "Epoch 38/200\n",
      "576/576 [==============================] - 0s - loss: 0.5436 - acc: 0.7378 - val_loss: 0.5660 - val_acc: 0.7396\n",
      "Epoch 39/200\n",
      "576/576 [==============================] - 0s - loss: 0.5413 - acc: 0.7361 - val_loss: 0.5642 - val_acc: 0.7344\n",
      "Epoch 40/200\n",
      "576/576 [==============================] - 0s - loss: 0.5391 - acc: 0.7378 - val_loss: 0.5625 - val_acc: 0.7344\n",
      "Epoch 41/200\n",
      "576/576 [==============================] - 0s - loss: 0.5370 - acc: 0.7396 - val_loss: 0.5608 - val_acc: 0.7396\n",
      "Epoch 42/200\n",
      "576/576 [==============================] - 0s - loss: 0.5349 - acc: 0.7431 - val_loss: 0.5592 - val_acc: 0.7396\n",
      "Epoch 43/200\n",
      "576/576 [==============================] - 0s - loss: 0.5328 - acc: 0.7431 - val_loss: 0.5576 - val_acc: 0.7396\n",
      "Epoch 44/200\n",
      "576/576 [==============================] - 0s - loss: 0.5309 - acc: 0.7431 - val_loss: 0.5561 - val_acc: 0.7396\n",
      "Epoch 45/200\n",
      "576/576 [==============================] - 0s - loss: 0.5290 - acc: 0.7448 - val_loss: 0.5546 - val_acc: 0.7396\n",
      "Epoch 46/200\n",
      "576/576 [==============================] - 0s - loss: 0.5271 - acc: 0.7483 - val_loss: 0.5531 - val_acc: 0.7448\n",
      "Epoch 47/200\n",
      "576/576 [==============================] - 0s - loss: 0.5253 - acc: 0.7483 - val_loss: 0.5518 - val_acc: 0.7500\n",
      "Epoch 48/200\n",
      "576/576 [==============================] - 0s - loss: 0.5235 - acc: 0.7465 - val_loss: 0.5504 - val_acc: 0.7448\n",
      "Epoch 49/200\n",
      "576/576 [==============================] - 0s - loss: 0.5218 - acc: 0.7448 - val_loss: 0.5491 - val_acc: 0.7448\n",
      "Epoch 50/200\n",
      "576/576 [==============================] - 0s - loss: 0.5202 - acc: 0.7448 - val_loss: 0.5479 - val_acc: 0.7396\n",
      "Epoch 51/200\n",
      "576/576 [==============================] - 0s - loss: 0.5186 - acc: 0.7448 - val_loss: 0.5467 - val_acc: 0.7500\n",
      "Epoch 52/200\n",
      "576/576 [==============================] - 0s - loss: 0.5170 - acc: 0.7448 - val_loss: 0.5454 - val_acc: 0.7552\n",
      "Epoch 53/200\n",
      "576/576 [==============================] - 0s - loss: 0.5154 - acc: 0.7483 - val_loss: 0.5443 - val_acc: 0.7552\n",
      "Epoch 54/200\n",
      "576/576 [==============================] - 0s - loss: 0.5139 - acc: 0.7483 - val_loss: 0.5432 - val_acc: 0.7552\n",
      "Epoch 55/200\n",
      "576/576 [==============================] - 0s - loss: 0.5124 - acc: 0.7483 - val_loss: 0.5421 - val_acc: 0.7552\n",
      "Epoch 56/200\n",
      "576/576 [==============================] - 0s - loss: 0.5110 - acc: 0.7517 - val_loss: 0.5411 - val_acc: 0.7552\n",
      "Epoch 57/200\n",
      "576/576 [==============================] - 0s - loss: 0.5096 - acc: 0.7517 - val_loss: 0.5400 - val_acc: 0.7552\n",
      "Epoch 58/200\n",
      "576/576 [==============================] - 0s - loss: 0.5082 - acc: 0.7517 - val_loss: 0.5391 - val_acc: 0.7552\n",
      "Epoch 59/200\n",
      "576/576 [==============================] - 0s - loss: 0.5069 - acc: 0.7535 - val_loss: 0.5381 - val_acc: 0.7552\n",
      "Epoch 60/200\n",
      "576/576 [==============================] - 0s - loss: 0.5056 - acc: 0.7535 - val_loss: 0.5372 - val_acc: 0.7552\n",
      "Epoch 61/200\n",
      "576/576 [==============================] - 0s - loss: 0.5043 - acc: 0.7552 - val_loss: 0.5363 - val_acc: 0.7604\n",
      "Epoch 62/200\n",
      "576/576 [==============================] - 0s - loss: 0.5031 - acc: 0.7535 - val_loss: 0.5354 - val_acc: 0.7552\n",
      "Epoch 63/200\n",
      "576/576 [==============================] - 0s - loss: 0.5019 - acc: 0.7535 - val_loss: 0.5346 - val_acc: 0.7552\n",
      "Epoch 64/200\n",
      "576/576 [==============================] - 0s - loss: 0.5008 - acc: 0.7569 - val_loss: 0.5337 - val_acc: 0.7552\n",
      "Epoch 65/200\n",
      "576/576 [==============================] - 0s - loss: 0.4996 - acc: 0.7604 - val_loss: 0.5330 - val_acc: 0.7500\n",
      "Epoch 66/200\n",
      "576/576 [==============================] - 0s - loss: 0.4985 - acc: 0.7604 - val_loss: 0.5322 - val_acc: 0.7500\n",
      "Epoch 67/200\n",
      "576/576 [==============================] - 0s - loss: 0.4974 - acc: 0.7604 - val_loss: 0.5314 - val_acc: 0.7500\n",
      "Epoch 68/200\n",
      "576/576 [==============================] - 0s - loss: 0.4964 - acc: 0.7604 - val_loss: 0.5307 - val_acc: 0.7500\n",
      "Epoch 69/200\n",
      "576/576 [==============================] - 0s - loss: 0.4954 - acc: 0.7587 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 70/200\n",
      "576/576 [==============================] - 0s - loss: 0.4944 - acc: 0.7622 - val_loss: 0.5293 - val_acc: 0.7500\n",
      "Epoch 71/200\n",
      "576/576 [==============================] - 0s - loss: 0.4934 - acc: 0.7622 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 72/200\n",
      "576/576 [==============================] - 0s - loss: 0.4925 - acc: 0.7604 - val_loss: 0.5280 - val_acc: 0.7500\n",
      "Epoch 73/200\n",
      "576/576 [==============================] - 0s - loss: 0.4915 - acc: 0.7622 - val_loss: 0.5274 - val_acc: 0.7500\n",
      "Epoch 74/200\n",
      "576/576 [==============================] - 0s - loss: 0.4906 - acc: 0.7622 - val_loss: 0.5268 - val_acc: 0.7448\n",
      "Epoch 75/200\n",
      "576/576 [==============================] - 0s - loss: 0.4897 - acc: 0.7604 - val_loss: 0.5263 - val_acc: 0.7448\n",
      "Epoch 76/200\n",
      "576/576 [==============================] - 0s - loss: 0.4889 - acc: 0.7639 - val_loss: 0.5257 - val_acc: 0.7448\n",
      "Epoch 77/200\n",
      "576/576 [==============================] - 0s - loss: 0.4881 - acc: 0.7656 - val_loss: 0.5252 - val_acc: 0.7396\n",
      "Epoch 78/200\n",
      "576/576 [==============================] - 0s - loss: 0.4872 - acc: 0.7656 - val_loss: 0.5246 - val_acc: 0.7396\n",
      "Epoch 79/200\n",
      "576/576 [==============================] - 0s - loss: 0.4864 - acc: 0.7656 - val_loss: 0.5241 - val_acc: 0.7396\n",
      "Epoch 80/200\n",
      "576/576 [==============================] - 0s - loss: 0.4856 - acc: 0.7674 - val_loss: 0.5236 - val_acc: 0.7396\n",
      "Epoch 81/200\n",
      "576/576 [==============================] - 0s - loss: 0.4849 - acc: 0.7674 - val_loss: 0.5231 - val_acc: 0.7396\n",
      "Epoch 82/200\n",
      "576/576 [==============================] - 0s - loss: 0.4841 - acc: 0.7674 - val_loss: 0.5226 - val_acc: 0.7396\n",
      "Epoch 83/200\n",
      "576/576 [==============================] - 0s - loss: 0.4834 - acc: 0.7656 - val_loss: 0.5222 - val_acc: 0.7396\n",
      "Epoch 84/200\n",
      "576/576 [==============================] - 0s - loss: 0.4827 - acc: 0.7622 - val_loss: 0.5217 - val_acc: 0.7396\n",
      "Epoch 85/200\n",
      "576/576 [==============================] - 0s - loss: 0.4820 - acc: 0.7622 - val_loss: 0.5213 - val_acc: 0.7396\n",
      "Epoch 86/200\n",
      "576/576 [==============================] - 0s - loss: 0.4813 - acc: 0.7622 - val_loss: 0.5209 - val_acc: 0.7396\n",
      "Epoch 87/200\n",
      "576/576 [==============================] - 0s - loss: 0.4806 - acc: 0.7639 - val_loss: 0.5205 - val_acc: 0.7396\n",
      "Epoch 88/200\n",
      "576/576 [==============================] - 0s - loss: 0.4800 - acc: 0.7656 - val_loss: 0.5200 - val_acc: 0.7396\n",
      "Epoch 89/200\n",
      "576/576 [==============================] - 0s - loss: 0.4793 - acc: 0.7656 - val_loss: 0.5197 - val_acc: 0.7396\n",
      "Epoch 90/200\n",
      "576/576 [==============================] - 0s - loss: 0.4787 - acc: 0.7656 - val_loss: 0.5193 - val_acc: 0.7396\n",
      "Epoch 91/200\n",
      "576/576 [==============================] - 0s - loss: 0.4781 - acc: 0.7656 - val_loss: 0.5189 - val_acc: 0.7396\n",
      "Epoch 92/200\n",
      "576/576 [==============================] - 0s - loss: 0.4775 - acc: 0.7674 - val_loss: 0.5186 - val_acc: 0.7448\n",
      "Epoch 93/200\n",
      "576/576 [==============================] - 0s - loss: 0.4769 - acc: 0.7674 - val_loss: 0.5182 - val_acc: 0.7448\n",
      "Epoch 94/200\n",
      "576/576 [==============================] - 0s - loss: 0.4764 - acc: 0.7674 - val_loss: 0.5179 - val_acc: 0.7448\n",
      "Epoch 95/200\n",
      "576/576 [==============================] - 0s - loss: 0.4758 - acc: 0.7691 - val_loss: 0.5176 - val_acc: 0.7500\n",
      "Epoch 96/200\n",
      "576/576 [==============================] - 0s - loss: 0.4752 - acc: 0.7691 - val_loss: 0.5173 - val_acc: 0.7552\n",
      "Epoch 97/200\n",
      "576/576 [==============================] - 0s - loss: 0.4746 - acc: 0.7708 - val_loss: 0.5170 - val_acc: 0.7552\n",
      "Epoch 98/200\n",
      "576/576 [==============================] - 0s - loss: 0.4741 - acc: 0.7691 - val_loss: 0.5167 - val_acc: 0.7552\n",
      "Epoch 99/200\n",
      "576/576 [==============================] - 0s - loss: 0.4736 - acc: 0.7708 - val_loss: 0.5164 - val_acc: 0.7552\n",
      "Epoch 100/200\n",
      "576/576 [==============================] - 0s - loss: 0.4730 - acc: 0.7708 - val_loss: 0.5161 - val_acc: 0.7500\n",
      "Epoch 101/200\n",
      "576/576 [==============================] - 0s - loss: 0.4726 - acc: 0.7708 - val_loss: 0.5158 - val_acc: 0.7500\n",
      "Epoch 102/200\n",
      "576/576 [==============================] - 0s - loss: 0.4720 - acc: 0.7726 - val_loss: 0.5156 - val_acc: 0.7500\n",
      "Epoch 103/200\n",
      "576/576 [==============================] - 0s - loss: 0.4715 - acc: 0.7708 - val_loss: 0.5153 - val_acc: 0.7500\n",
      "Epoch 104/200\n",
      "576/576 [==============================] - 0s - loss: 0.4710 - acc: 0.7708 - val_loss: 0.5151 - val_acc: 0.7500\n",
      "Epoch 105/200\n",
      "576/576 [==============================] - 0s - loss: 0.4706 - acc: 0.7726 - val_loss: 0.5149 - val_acc: 0.7448\n",
      "Epoch 106/200\n",
      "576/576 [==============================] - 0s - loss: 0.4701 - acc: 0.7743 - val_loss: 0.5146 - val_acc: 0.7448\n",
      "Epoch 107/200\n",
      "576/576 [==============================] - 0s - loss: 0.4697 - acc: 0.7743 - val_loss: 0.5144 - val_acc: 0.7500\n",
      "Epoch 108/200\n",
      "576/576 [==============================] - 0s - loss: 0.4692 - acc: 0.7743 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 109/200\n",
      "576/576 [==============================] - 0s - loss: 0.4688 - acc: 0.7743 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 110/200\n",
      "576/576 [==============================] - 0s - loss: 0.4684 - acc: 0.7743 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 111/200\n",
      "576/576 [==============================] - 0s - loss: 0.4680 - acc: 0.7743 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 112/200\n",
      "576/576 [==============================] - 0s - loss: 0.4676 - acc: 0.7743 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 113/200\n",
      "576/576 [==============================] - 0s - loss: 0.4672 - acc: 0.7743 - val_loss: 0.5133 - val_acc: 0.7500\n",
      "Epoch 114/200\n",
      "576/576 [==============================] - 0s - loss: 0.4668 - acc: 0.7743 - val_loss: 0.5131 - val_acc: 0.7500\n",
      "Epoch 115/200\n",
      "576/576 [==============================] - 0s - loss: 0.4664 - acc: 0.7743 - val_loss: 0.5129 - val_acc: 0.7500\n",
      "Epoch 116/200\n",
      "576/576 [==============================] - 0s - loss: 0.4660 - acc: 0.7743 - val_loss: 0.5128 - val_acc: 0.7500\n",
      "Epoch 117/200\n",
      "576/576 [==============================] - 0s - loss: 0.4657 - acc: 0.7743 - val_loss: 0.5126 - val_acc: 0.7500\n",
      "Epoch 118/200\n",
      "576/576 [==============================] - 0s - loss: 0.4653 - acc: 0.7743 - val_loss: 0.5124 - val_acc: 0.7500\n",
      "Epoch 119/200\n",
      "576/576 [==============================] - 0s - loss: 0.4650 - acc: 0.7743 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 120/200\n",
      "576/576 [==============================] - 0s - loss: 0.4646 - acc: 0.7743 - val_loss: 0.5121 - val_acc: 0.7500\n",
      "Epoch 121/200\n",
      "576/576 [==============================] - 0s - loss: 0.4642 - acc: 0.7743 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 122/200\n",
      "576/576 [==============================] - 0s - loss: 0.4639 - acc: 0.7743 - val_loss: 0.5118 - val_acc: 0.7448\n",
      "Epoch 123/200\n",
      "576/576 [==============================] - 0s - loss: 0.4636 - acc: 0.7743 - val_loss: 0.5117 - val_acc: 0.7448\n",
      "Epoch 124/200\n",
      "576/576 [==============================] - 0s - loss: 0.4633 - acc: 0.7743 - val_loss: 0.5116 - val_acc: 0.7448\n",
      "Epoch 125/200\n",
      "576/576 [==============================] - 0s - loss: 0.4630 - acc: 0.7743 - val_loss: 0.5115 - val_acc: 0.7448\n",
      "Epoch 126/200\n",
      "576/576 [==============================] - 0s - loss: 0.4627 - acc: 0.7743 - val_loss: 0.5113 - val_acc: 0.7448\n",
      "Epoch 127/200\n",
      "576/576 [==============================] - 0s - loss: 0.4624 - acc: 0.7760 - val_loss: 0.5112 - val_acc: 0.7448\n",
      "Epoch 128/200\n",
      "576/576 [==============================] - 0s - loss: 0.4621 - acc: 0.7760 - val_loss: 0.5111 - val_acc: 0.7448\n",
      "Epoch 129/200\n",
      "576/576 [==============================] - 0s - loss: 0.4617 - acc: 0.7760 - val_loss: 0.5110 - val_acc: 0.7448\n",
      "Epoch 130/200\n",
      "576/576 [==============================] - 0s - loss: 0.4614 - acc: 0.7760 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 131/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s - loss: 0.4612 - acc: 0.7778 - val_loss: 0.5107 - val_acc: 0.7448\n",
      "Epoch 132/200\n",
      "576/576 [==============================] - 0s - loss: 0.4609 - acc: 0.7778 - val_loss: 0.5106 - val_acc: 0.7448\n",
      "Epoch 133/200\n",
      "576/576 [==============================] - 0s - loss: 0.4606 - acc: 0.7778 - val_loss: 0.5105 - val_acc: 0.7448\n",
      "Epoch 134/200\n",
      "576/576 [==============================] - 0s - loss: 0.4604 - acc: 0.7778 - val_loss: 0.5105 - val_acc: 0.7396\n",
      "Epoch 135/200\n",
      "576/576 [==============================] - 0s - loss: 0.4601 - acc: 0.7778 - val_loss: 0.5104 - val_acc: 0.7448\n",
      "Epoch 136/200\n",
      "576/576 [==============================] - 0s - loss: 0.4599 - acc: 0.7795 - val_loss: 0.5103 - val_acc: 0.7448\n",
      "Epoch 137/200\n",
      "576/576 [==============================] - 0s - loss: 0.4596 - acc: 0.7795 - val_loss: 0.5102 - val_acc: 0.7448\n",
      "Epoch 138/200\n",
      "576/576 [==============================] - 0s - loss: 0.4594 - acc: 0.7795 - val_loss: 0.5101 - val_acc: 0.7448\n",
      "Epoch 139/200\n",
      "576/576 [==============================] - 0s - loss: 0.4591 - acc: 0.7795 - val_loss: 0.5100 - val_acc: 0.7448\n",
      "Epoch 140/200\n",
      "576/576 [==============================] - 0s - loss: 0.4589 - acc: 0.7795 - val_loss: 0.5099 - val_acc: 0.7448\n",
      "Epoch 141/200\n",
      "576/576 [==============================] - 0s - loss: 0.4587 - acc: 0.7795 - val_loss: 0.5098 - val_acc: 0.7448\n",
      "Epoch 142/200\n",
      "576/576 [==============================] - 0s - loss: 0.4584 - acc: 0.7795 - val_loss: 0.5098 - val_acc: 0.7448\n",
      "Epoch 143/200\n",
      "576/576 [==============================] - 0s - loss: 0.4582 - acc: 0.7795 - val_loss: 0.5097 - val_acc: 0.7500\n",
      "Epoch 144/200\n",
      "576/576 [==============================] - 0s - loss: 0.4580 - acc: 0.7795 - val_loss: 0.5096 - val_acc: 0.7500\n",
      "Epoch 145/200\n",
      "576/576 [==============================] - 0s - loss: 0.4577 - acc: 0.7812 - val_loss: 0.5096 - val_acc: 0.7500\n",
      "Epoch 146/200\n",
      "576/576 [==============================] - 0s - loss: 0.4575 - acc: 0.7812 - val_loss: 0.5095 - val_acc: 0.7500\n",
      "Epoch 147/200\n",
      "576/576 [==============================] - 0s - loss: 0.4573 - acc: 0.7812 - val_loss: 0.5094 - val_acc: 0.7500\n",
      "Epoch 148/200\n",
      "576/576 [==============================] - 0s - loss: 0.4571 - acc: 0.7795 - val_loss: 0.5094 - val_acc: 0.7500\n",
      "Epoch 149/200\n",
      "576/576 [==============================] - 0s - loss: 0.4569 - acc: 0.7812 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 150/200\n",
      "576/576 [==============================] - 0s - loss: 0.4567 - acc: 0.7812 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 151/200\n",
      "576/576 [==============================] - 0s - loss: 0.4565 - acc: 0.7812 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 152/200\n",
      "576/576 [==============================] - 0s - loss: 0.4563 - acc: 0.7812 - val_loss: 0.5092 - val_acc: 0.7500\n",
      "Epoch 153/200\n",
      "576/576 [==============================] - 0s - loss: 0.4561 - acc: 0.7812 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 154/200\n",
      "576/576 [==============================] - 0s - loss: 0.4559 - acc: 0.7812 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 155/200\n",
      "576/576 [==============================] - 0s - loss: 0.4557 - acc: 0.7812 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 156/200\n",
      "576/576 [==============================] - 0s - loss: 0.4555 - acc: 0.7812 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 157/200\n",
      "576/576 [==============================] - 0s - loss: 0.4554 - acc: 0.7812 - val_loss: 0.5090 - val_acc: 0.7552\n",
      "Epoch 158/200\n",
      "576/576 [==============================] - 0s - loss: 0.4552 - acc: 0.7812 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 159/200\n",
      "576/576 [==============================] - 0s - loss: 0.4550 - acc: 0.7812 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 160/200\n",
      "576/576 [==============================] - 0s - loss: 0.4549 - acc: 0.7812 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 161/200\n",
      "576/576 [==============================] - 0s - loss: 0.4547 - acc: 0.7812 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 162/200\n",
      "576/576 [==============================] - 0s - loss: 0.4545 - acc: 0.7812 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 163/200\n",
      "576/576 [==============================] - 0s - loss: 0.4543 - acc: 0.7812 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 164/200\n",
      "576/576 [==============================] - 0s - loss: 0.4542 - acc: 0.7812 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 165/200\n",
      "576/576 [==============================] - 0s - loss: 0.4540 - acc: 0.7812 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 166/200\n",
      "576/576 [==============================] - 0s - loss: 0.4538 - acc: 0.7812 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 167/200\n",
      "576/576 [==============================] - 0s - loss: 0.4537 - acc: 0.7812 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 168/200\n",
      "576/576 [==============================] - 0s - loss: 0.4535 - acc: 0.7812 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 169/200\n",
      "576/576 [==============================] - 0s - loss: 0.4534 - acc: 0.7812 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 170/200\n",
      "576/576 [==============================] - 0s - loss: 0.4532 - acc: 0.7830 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 171/200\n",
      "576/576 [==============================] - 0s - loss: 0.4531 - acc: 0.7812 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 172/200\n",
      "576/576 [==============================] - 0s - loss: 0.4529 - acc: 0.7830 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 173/200\n",
      "576/576 [==============================] - 0s - loss: 0.4528 - acc: 0.7830 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 174/200\n",
      "576/576 [==============================] - 0s - loss: 0.4527 - acc: 0.7830 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 175/200\n",
      "576/576 [==============================] - 0s - loss: 0.4525 - acc: 0.7812 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 176/200\n",
      "576/576 [==============================] - 0s - loss: 0.4524 - acc: 0.7830 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 177/200\n",
      "576/576 [==============================] - 0s - loss: 0.4522 - acc: 0.7847 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 178/200\n",
      "576/576 [==============================] - 0s - loss: 0.4521 - acc: 0.7830 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 179/200\n",
      "576/576 [==============================] - 0s - loss: 0.4519 - acc: 0.7847 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 180/200\n",
      "576/576 [==============================] - 0s - loss: 0.4518 - acc: 0.7847 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 181/200\n",
      "576/576 [==============================] - 0s - loss: 0.4517 - acc: 0.7830 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 182/200\n",
      "576/576 [==============================] - 0s - loss: 0.4516 - acc: 0.7847 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 183/200\n",
      "576/576 [==============================] - 0s - loss: 0.4515 - acc: 0.7847 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 184/200\n",
      "576/576 [==============================] - 0s - loss: 0.4513 - acc: 0.7830 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 185/200\n",
      "576/576 [==============================] - 0s - loss: 0.4512 - acc: 0.7830 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 186/200\n",
      "576/576 [==============================] - 0s - loss: 0.4511 - acc: 0.7830 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 187/200\n",
      "576/576 [==============================] - 0s - loss: 0.4509 - acc: 0.7830 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 188/200\n",
      "576/576 [==============================] - 0s - loss: 0.4508 - acc: 0.7830 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 189/200\n",
      "576/576 [==============================] - 0s - loss: 0.4507 - acc: 0.7830 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 190/200\n",
      "576/576 [==============================] - 0s - loss: 0.4506 - acc: 0.7830 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 191/200\n",
      "576/576 [==============================] - 0s - loss: 0.4505 - acc: 0.7830 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 192/200\n",
      "576/576 [==============================] - 0s - loss: 0.4504 - acc: 0.7830 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 193/200\n",
      "576/576 [==============================] - 0s - loss: 0.4502 - acc: 0.7830 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 194/200\n",
      "576/576 [==============================] - 0s - loss: 0.4501 - acc: 0.7830 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 195/200\n",
      "576/576 [==============================] - 0s - loss: 0.4501 - acc: 0.7847 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 196/200\n",
      "576/576 [==============================] - 0s - loss: 0.4499 - acc: 0.7847 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 197/200\n",
      "576/576 [==============================] - 0s - loss: 0.4499 - acc: 0.7847 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 198/200\n",
      "576/576 [==============================] - 0s - loss: 0.4497 - acc: 0.7847 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 199/200\n",
      "576/576 [==============================] - 0s - loss: 0.4496 - acc: 0.7847 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 200/200\n",
      "576/576 [==============================] - 0s - loss: 0.4495 - acc: 0.7847 - val_loss: 0.5086 - val_acc: 0.7604\n"
     ]
    }
   ],
   "source": [
    "# Fit(Train) the Model\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/192 [====>.........................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "## Like we did for the Random Forest, we generate two kinds of predictions\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the outputs to get a feel for how keras apis work.\n",
    "y_pred_class_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.59435862],\n",
       "       [ 0.70445299],\n",
       "       [ 0.25970057],\n",
       "       [ 0.17872553],\n",
       "       [ 0.20430985],\n",
       "       [ 0.4604139 ],\n",
       "       [ 0.03336516],\n",
       "       [ 0.42107245],\n",
       "       [ 0.96607256],\n",
       "       [ 0.20274091]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.760\n",
      "roc-auc is 0.813\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAxhuIMAECMoTgDABBj/j/VAQl7BBLj0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1c0368d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model performance and plot the roc curve\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some variation in exact numbers due to randomness, but you should get results in the same ballpark as the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `run_hist_1` object that was created, specifically its `history` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training loss and the validation loss over the different epochs and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c1c4e5470>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1c56cfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4494 - acc: 0.7847 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 2/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4493 - acc: 0.7847 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 3/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4492 - acc: 0.7847 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 4/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4491 - acc: 0.7847 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 5/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4490 - acc: 0.7847 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 6/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4489 - acc: 0.7847 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 7/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4488 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 8/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4487 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 9/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4486 - acc: 0.7847 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 10/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4485 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 11/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4484 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 12/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4483 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 13/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4482 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 14/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4481 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 15/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4480 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 16/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4479 - acc: 0.7865 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 17/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4478 - acc: 0.7865 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 18/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4477 - acc: 0.7865 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 19/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4477 - acc: 0.7865 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 20/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4476 - acc: 0.7865 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 21/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4475 - acc: 0.7865 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 22/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4474 - acc: 0.7865 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 23/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4473 - acc: 0.7882 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 24/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4472 - acc: 0.7865 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 25/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4471 - acc: 0.7865 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 26/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4470 - acc: 0.7865 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 27/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4469 - acc: 0.7882 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 28/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4468 - acc: 0.7865 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 29/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4468 - acc: 0.7865 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 30/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4467 - acc: 0.7865 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 31/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4466 - acc: 0.7865 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 32/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4465 - acc: 0.7882 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 33/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4464 - acc: 0.7899 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 34/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4464 - acc: 0.7899 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 35/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4463 - acc: 0.7882 - val_loss: 0.5090 - val_acc: 0.7604\n",
      "Epoch 36/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4462 - acc: 0.7899 - val_loss: 0.5090 - val_acc: 0.7604\n",
      "Epoch 37/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4461 - acc: 0.7899 - val_loss: 0.5090 - val_acc: 0.7604\n",
      "Epoch 38/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4461 - acc: 0.7899 - val_loss: 0.5090 - val_acc: 0.7604\n",
      "Epoch 39/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4460 - acc: 0.7899 - val_loss: 0.5090 - val_acc: 0.7604\n",
      "Epoch 40/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4459 - acc: 0.7899 - val_loss: 0.5090 - val_acc: 0.7604\n",
      "Epoch 41/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4458 - acc: 0.7899 - val_loss: 0.5090 - val_acc: 0.7604\n",
      "Epoch 42/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4457 - acc: 0.7899 - val_loss: 0.5091 - val_acc: 0.7604\n",
      "Epoch 43/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4456 - acc: 0.7899 - val_loss: 0.5091 - val_acc: 0.7604\n",
      "Epoch 44/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4456 - acc: 0.7899 - val_loss: 0.5091 - val_acc: 0.7604\n",
      "Epoch 45/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4455 - acc: 0.7899 - val_loss: 0.5091 - val_acc: 0.7604\n",
      "Epoch 46/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4454 - acc: 0.7882 - val_loss: 0.5091 - val_acc: 0.7604\n",
      "Epoch 47/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4453 - acc: 0.7882 - val_loss: 0.5091 - val_acc: 0.7604\n",
      "Epoch 48/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4453 - acc: 0.7882 - val_loss: 0.5091 - val_acc: 0.7604\n",
      "Epoch 49/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4452 - acc: 0.7882 - val_loss: 0.5092 - val_acc: 0.7604\n",
      "Epoch 50/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4451 - acc: 0.7882 - val_loss: 0.5092 - val_acc: 0.7604\n",
      "Epoch 51/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4450 - acc: 0.7882 - val_loss: 0.5092 - val_acc: 0.7552\n",
      "Epoch 52/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4450 - acc: 0.7882 - val_loss: 0.5092 - val_acc: 0.7552\n",
      "Epoch 53/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4449 - acc: 0.7882 - val_loss: 0.5092 - val_acc: 0.7552\n",
      "Epoch 54/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4448 - acc: 0.7882 - val_loss: 0.5092 - val_acc: 0.7552\n",
      "Epoch 55/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4447 - acc: 0.7882 - val_loss: 0.5092 - val_acc: 0.7552\n",
      "Epoch 56/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4447 - acc: 0.7882 - val_loss: 0.5092 - val_acc: 0.7552\n",
      "Epoch 57/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4446 - acc: 0.7882 - val_loss: 0.5092 - val_acc: 0.7552\n",
      "Epoch 58/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4446 - acc: 0.7882 - val_loss: 0.5093 - val_acc: 0.7552\n",
      "Epoch 59/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4445 - acc: 0.7882 - val_loss: 0.5093 - val_acc: 0.7552\n",
      "Epoch 60/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4444 - acc: 0.7882 - val_loss: 0.5093 - val_acc: 0.7552\n",
      "Epoch 61/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4443 - acc: 0.7882 - val_loss: 0.5093 - val_acc: 0.7552\n",
      "Epoch 62/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4442 - acc: 0.7882 - val_loss: 0.5093 - val_acc: 0.7552\n",
      "Epoch 63/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4442 - acc: 0.7882 - val_loss: 0.5093 - val_acc: 0.7552\n",
      "Epoch 64/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4441 - acc: 0.7882 - val_loss: 0.5093 - val_acc: 0.7552\n",
      "Epoch 65/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4440 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7552\n",
      "Epoch 66/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4440 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7552\n",
      "Epoch 67/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4439 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7552\n",
      "Epoch 68/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4438 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7552\n",
      "Epoch 69/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4437 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7552\n",
      "Epoch 70/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4437 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7552\n",
      "Epoch 71/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4436 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7552\n",
      "Epoch 72/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4436 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7552\n",
      "Epoch 73/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4435 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7552\n",
      "Epoch 74/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4434 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7552\n",
      "Epoch 75/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4434 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7552\n",
      "Epoch 76/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4433 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7552\n",
      "Epoch 77/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4432 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7552\n",
      "Epoch 78/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4432 - acc: 0.7865 - val_loss: 0.5095 - val_acc: 0.7552\n",
      "Epoch 79/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4431 - acc: 0.7865 - val_loss: 0.5096 - val_acc: 0.7552\n",
      "Epoch 80/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4430 - acc: 0.7865 - val_loss: 0.5096 - val_acc: 0.7552\n",
      "Epoch 81/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4430 - acc: 0.7865 - val_loss: 0.5096 - val_acc: 0.7552\n",
      "Epoch 82/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4429 - acc: 0.7865 - val_loss: 0.5096 - val_acc: 0.7552\n",
      "Epoch 83/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4428 - acc: 0.7865 - val_loss: 0.5096 - val_acc: 0.7552\n",
      "Epoch 84/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4428 - acc: 0.7865 - val_loss: 0.5096 - val_acc: 0.7552\n",
      "Epoch 85/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4427 - acc: 0.7865 - val_loss: 0.5096 - val_acc: 0.7552\n",
      "Epoch 86/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4426 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7552\n",
      "Epoch 87/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4426 - acc: 0.7847 - val_loss: 0.5097 - val_acc: 0.7552\n",
      "Epoch 88/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4425 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7552\n",
      "Epoch 89/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4424 - acc: 0.7847 - val_loss: 0.5097 - val_acc: 0.7552\n",
      "Epoch 90/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4424 - acc: 0.7847 - val_loss: 0.5097 - val_acc: 0.7552\n",
      "Epoch 91/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4423 - acc: 0.7847 - val_loss: 0.5098 - val_acc: 0.7552\n",
      "Epoch 92/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4422 - acc: 0.7865 - val_loss: 0.5098 - val_acc: 0.7552\n",
      "Epoch 93/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4422 - acc: 0.7847 - val_loss: 0.5098 - val_acc: 0.7552\n",
      "Epoch 94/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4422 - acc: 0.7847 - val_loss: 0.5098 - val_acc: 0.7552\n",
      "Epoch 95/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4421 - acc: 0.7847 - val_loss: 0.5098 - val_acc: 0.7552\n",
      "Epoch 96/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4420 - acc: 0.7847 - val_loss: 0.5099 - val_acc: 0.7552\n",
      "Epoch 97/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4419 - acc: 0.7847 - val_loss: 0.5099 - val_acc: 0.7552\n",
      "Epoch 98/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4419 - acc: 0.7847 - val_loss: 0.5099 - val_acc: 0.7552\n",
      "Epoch 99/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4418 - acc: 0.7865 - val_loss: 0.5099 - val_acc: 0.7552\n",
      "Epoch 100/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4417 - acc: 0.7865 - val_loss: 0.5100 - val_acc: 0.7552\n",
      "Epoch 101/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4417 - acc: 0.7847 - val_loss: 0.5100 - val_acc: 0.7604\n",
      "Epoch 102/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4417 - acc: 0.7865 - val_loss: 0.5100 - val_acc: 0.7604\n",
      "Epoch 103/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4416 - acc: 0.7865 - val_loss: 0.5100 - val_acc: 0.7604\n",
      "Epoch 104/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4415 - acc: 0.7847 - val_loss: 0.5100 - val_acc: 0.7552\n",
      "Epoch 105/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4415 - acc: 0.7865 - val_loss: 0.5100 - val_acc: 0.7552\n",
      "Epoch 106/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4414 - acc: 0.7865 - val_loss: 0.5101 - val_acc: 0.7552\n",
      "Epoch 107/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4413 - acc: 0.7847 - val_loss: 0.5101 - val_acc: 0.7552\n",
      "Epoch 108/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4413 - acc: 0.7847 - val_loss: 0.5101 - val_acc: 0.7552\n",
      "Epoch 109/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4412 - acc: 0.7865 - val_loss: 0.5101 - val_acc: 0.7552\n",
      "Epoch 110/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4411 - acc: 0.7865 - val_loss: 0.5101 - val_acc: 0.7552\n",
      "Epoch 111/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4411 - acc: 0.7882 - val_loss: 0.5102 - val_acc: 0.7552\n",
      "Epoch 112/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4410 - acc: 0.7865 - val_loss: 0.5102 - val_acc: 0.7552\n",
      "Epoch 113/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4410 - acc: 0.7865 - val_loss: 0.5102 - val_acc: 0.7552\n",
      "Epoch 114/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4409 - acc: 0.7865 - val_loss: 0.5102 - val_acc: 0.7552\n",
      "Epoch 115/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4409 - acc: 0.7865 - val_loss: 0.5102 - val_acc: 0.7552\n",
      "Epoch 116/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4408 - acc: 0.7865 - val_loss: 0.5103 - val_acc: 0.7552\n",
      "Epoch 117/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4407 - acc: 0.7847 - val_loss: 0.5103 - val_acc: 0.7552\n",
      "Epoch 118/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4407 - acc: 0.7847 - val_loss: 0.5103 - val_acc: 0.7552\n",
      "Epoch 119/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4406 - acc: 0.7865 - val_loss: 0.5103 - val_acc: 0.7552\n",
      "Epoch 120/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4406 - acc: 0.7847 - val_loss: 0.5103 - val_acc: 0.7552\n",
      "Epoch 121/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4405 - acc: 0.7847 - val_loss: 0.5104 - val_acc: 0.7552\n",
      "Epoch 122/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4405 - acc: 0.7865 - val_loss: 0.5104 - val_acc: 0.7552\n",
      "Epoch 123/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4404 - acc: 0.7865 - val_loss: 0.5104 - val_acc: 0.7552\n",
      "Epoch 124/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4403 - acc: 0.7847 - val_loss: 0.5104 - val_acc: 0.7552\n",
      "Epoch 125/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4403 - acc: 0.7847 - val_loss: 0.5104 - val_acc: 0.7552\n",
      "Epoch 126/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4402 - acc: 0.7847 - val_loss: 0.5105 - val_acc: 0.7552\n",
      "Epoch 127/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4402 - acc: 0.7847 - val_loss: 0.5105 - val_acc: 0.7552\n",
      "Epoch 128/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4401 - acc: 0.7865 - val_loss: 0.5105 - val_acc: 0.7552\n",
      "Epoch 129/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s - loss: 0.4401 - acc: 0.7865 - val_loss: 0.5105 - val_acc: 0.7552\n",
      "Epoch 130/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4400 - acc: 0.7847 - val_loss: 0.5105 - val_acc: 0.7552\n",
      "Epoch 131/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4400 - acc: 0.7847 - val_loss: 0.5105 - val_acc: 0.7552\n",
      "Epoch 132/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4399 - acc: 0.7847 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 133/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4398 - acc: 0.7865 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 134/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4398 - acc: 0.7847 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 135/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4397 - acc: 0.7847 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 136/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4397 - acc: 0.7847 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 137/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4396 - acc: 0.7847 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 138/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4396 - acc: 0.7847 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 139/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4395 - acc: 0.7847 - val_loss: 0.5107 - val_acc: 0.7552\n",
      "Epoch 140/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4395 - acc: 0.7847 - val_loss: 0.5107 - val_acc: 0.7552\n",
      "Epoch 141/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4394 - acc: 0.7847 - val_loss: 0.5107 - val_acc: 0.7552\n",
      "Epoch 142/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4394 - acc: 0.7847 - val_loss: 0.5107 - val_acc: 0.7552\n",
      "Epoch 143/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4393 - acc: 0.7847 - val_loss: 0.5107 - val_acc: 0.7552\n",
      "Epoch 144/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4392 - acc: 0.7847 - val_loss: 0.5107 - val_acc: 0.7552\n",
      "Epoch 145/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4392 - acc: 0.7847 - val_loss: 0.5107 - val_acc: 0.7552\n",
      "Epoch 146/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4391 - acc: 0.7847 - val_loss: 0.5107 - val_acc: 0.7552\n",
      "Epoch 147/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4391 - acc: 0.7847 - val_loss: 0.5107 - val_acc: 0.7552\n",
      "Epoch 148/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4390 - acc: 0.7847 - val_loss: 0.5108 - val_acc: 0.7552\n",
      "Epoch 149/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4390 - acc: 0.7847 - val_loss: 0.5108 - val_acc: 0.7552\n",
      "Epoch 150/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4389 - acc: 0.7847 - val_loss: 0.5108 - val_acc: 0.7552\n",
      "Epoch 151/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4389 - acc: 0.7847 - val_loss: 0.5108 - val_acc: 0.7552\n",
      "Epoch 152/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4388 - acc: 0.7847 - val_loss: 0.5108 - val_acc: 0.7552\n",
      "Epoch 153/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4388 - acc: 0.7847 - val_loss: 0.5108 - val_acc: 0.7552\n",
      "Epoch 154/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4387 - acc: 0.7847 - val_loss: 0.5108 - val_acc: 0.7552\n",
      "Epoch 155/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4387 - acc: 0.7847 - val_loss: 0.5108 - val_acc: 0.7552\n",
      "Epoch 156/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4386 - acc: 0.7847 - val_loss: 0.5108 - val_acc: 0.7552\n",
      "Epoch 157/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4386 - acc: 0.7847 - val_loss: 0.5108 - val_acc: 0.7552\n",
      "Epoch 158/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4385 - acc: 0.7847 - val_loss: 0.5108 - val_acc: 0.7552\n",
      "Epoch 159/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4385 - acc: 0.7847 - val_loss: 0.5108 - val_acc: 0.7552\n",
      "Epoch 160/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4384 - acc: 0.7847 - val_loss: 0.5108 - val_acc: 0.7552\n",
      "Epoch 161/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4384 - acc: 0.7847 - val_loss: 0.5108 - val_acc: 0.7552\n",
      "Epoch 162/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4383 - acc: 0.7847 - val_loss: 0.5109 - val_acc: 0.7552\n",
      "Epoch 163/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4383 - acc: 0.7847 - val_loss: 0.5109 - val_acc: 0.7552\n",
      "Epoch 164/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4382 - acc: 0.7847 - val_loss: 0.5109 - val_acc: 0.7552\n",
      "Epoch 165/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4382 - acc: 0.7847 - val_loss: 0.5109 - val_acc: 0.7552\n",
      "Epoch 166/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4381 - acc: 0.7847 - val_loss: 0.5109 - val_acc: 0.7552\n",
      "Epoch 167/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4381 - acc: 0.7847 - val_loss: 0.5109 - val_acc: 0.7552\n",
      "Epoch 168/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4380 - acc: 0.7847 - val_loss: 0.5109 - val_acc: 0.7552\n",
      "Epoch 169/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4380 - acc: 0.7847 - val_loss: 0.5109 - val_acc: 0.7552\n",
      "Epoch 170/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4379 - acc: 0.7847 - val_loss: 0.5109 - val_acc: 0.7552\n",
      "Epoch 171/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4379 - acc: 0.7847 - val_loss: 0.5109 - val_acc: 0.7552\n",
      "Epoch 172/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4378 - acc: 0.7847 - val_loss: 0.5109 - val_acc: 0.7552\n",
      "Epoch 173/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4378 - acc: 0.7847 - val_loss: 0.5110 - val_acc: 0.7552\n",
      "Epoch 174/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4377 - acc: 0.7847 - val_loss: 0.5110 - val_acc: 0.7552\n",
      "Epoch 175/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4377 - acc: 0.7847 - val_loss: 0.5110 - val_acc: 0.7552\n",
      "Epoch 176/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4376 - acc: 0.7847 - val_loss: 0.5110 - val_acc: 0.7552\n",
      "Epoch 177/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4376 - acc: 0.7847 - val_loss: 0.5110 - val_acc: 0.7552\n",
      "Epoch 178/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4376 - acc: 0.7847 - val_loss: 0.5110 - val_acc: 0.7552\n",
      "Epoch 179/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4375 - acc: 0.7847 - val_loss: 0.5110 - val_acc: 0.7552\n",
      "Epoch 180/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4374 - acc: 0.7847 - val_loss: 0.5110 - val_acc: 0.7552\n",
      "Epoch 181/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4374 - acc: 0.7847 - val_loss: 0.5110 - val_acc: 0.7552\n",
      "Epoch 182/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4374 - acc: 0.7847 - val_loss: 0.5110 - val_acc: 0.7552\n",
      "Epoch 183/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4373 - acc: 0.7847 - val_loss: 0.5110 - val_acc: 0.7552\n",
      "Epoch 184/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4373 - acc: 0.7847 - val_loss: 0.5110 - val_acc: 0.7552\n",
      "Epoch 185/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4372 - acc: 0.7847 - val_loss: 0.5111 - val_acc: 0.7552\n",
      "Epoch 186/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4372 - acc: 0.7847 - val_loss: 0.5111 - val_acc: 0.7552\n",
      "Epoch 187/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4371 - acc: 0.7847 - val_loss: 0.5111 - val_acc: 0.7552\n",
      "Epoch 188/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4371 - acc: 0.7847 - val_loss: 0.5111 - val_acc: 0.7552\n",
      "Epoch 189/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4371 - acc: 0.7847 - val_loss: 0.5111 - val_acc: 0.7552\n",
      "Epoch 190/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4370 - acc: 0.7847 - val_loss: 0.5111 - val_acc: 0.7552\n",
      "Epoch 191/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4370 - acc: 0.7847 - val_loss: 0.5111 - val_acc: 0.7552\n",
      "Epoch 192/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4369 - acc: 0.7847 - val_loss: 0.5111 - val_acc: 0.7552\n",
      "Epoch 193/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4369 - acc: 0.7847 - val_loss: 0.5112 - val_acc: 0.7552\n",
      "Epoch 194/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4368 - acc: 0.7847 - val_loss: 0.5112 - val_acc: 0.7552\n",
      "Epoch 195/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4368 - acc: 0.7847 - val_loss: 0.5112 - val_acc: 0.7552\n",
      "Epoch 196/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4367 - acc: 0.7847 - val_loss: 0.5112 - val_acc: 0.7552\n",
      "Epoch 197/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4367 - acc: 0.7847 - val_loss: 0.5112 - val_acc: 0.7552\n",
      "Epoch 198/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4367 - acc: 0.7847 - val_loss: 0.5112 - val_acc: 0.7552\n",
      "Epoch 199/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4366 - acc: 0.7847 - val_loss: 0.5113 - val_acc: 0.7552\n",
      "Epoch 200/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4365 - acc: 0.7847 - val_loss: 0.5113 - val_acc: 0.7552\n",
      "Epoch 201/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4365 - acc: 0.7847 - val_loss: 0.5113 - val_acc: 0.7552\n",
      "Epoch 202/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4365 - acc: 0.7847 - val_loss: 0.5113 - val_acc: 0.7552\n",
      "Epoch 203/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4364 - acc: 0.7847 - val_loss: 0.5113 - val_acc: 0.7552\n",
      "Epoch 204/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4364 - acc: 0.7847 - val_loss: 0.5113 - val_acc: 0.7552\n",
      "Epoch 205/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4364 - acc: 0.7847 - val_loss: 0.5114 - val_acc: 0.7552\n",
      "Epoch 206/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4363 - acc: 0.7847 - val_loss: 0.5114 - val_acc: 0.7552\n",
      "Epoch 207/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4363 - acc: 0.7847 - val_loss: 0.5114 - val_acc: 0.7552\n",
      "Epoch 208/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4362 - acc: 0.7847 - val_loss: 0.5114 - val_acc: 0.7552\n",
      "Epoch 209/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4362 - acc: 0.7847 - val_loss: 0.5114 - val_acc: 0.7552\n",
      "Epoch 210/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4362 - acc: 0.7847 - val_loss: 0.5114 - val_acc: 0.7604\n",
      "Epoch 211/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4361 - acc: 0.7847 - val_loss: 0.5114 - val_acc: 0.7604\n",
      "Epoch 212/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4361 - acc: 0.7847 - val_loss: 0.5114 - val_acc: 0.7604\n",
      "Epoch 213/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4361 - acc: 0.7847 - val_loss: 0.5115 - val_acc: 0.7604\n",
      "Epoch 214/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4360 - acc: 0.7847 - val_loss: 0.5115 - val_acc: 0.7604\n",
      "Epoch 215/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4359 - acc: 0.7847 - val_loss: 0.5115 - val_acc: 0.7604\n",
      "Epoch 216/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4359 - acc: 0.7847 - val_loss: 0.5115 - val_acc: 0.7604\n",
      "Epoch 217/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4359 - acc: 0.7847 - val_loss: 0.5115 - val_acc: 0.7604\n",
      "Epoch 218/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4358 - acc: 0.7847 - val_loss: 0.5115 - val_acc: 0.7604\n",
      "Epoch 219/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4358 - acc: 0.7847 - val_loss: 0.5115 - val_acc: 0.7604\n",
      "Epoch 220/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4357 - acc: 0.7847 - val_loss: 0.5115 - val_acc: 0.7604\n",
      "Epoch 221/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4357 - acc: 0.7847 - val_loss: 0.5116 - val_acc: 0.7604\n",
      "Epoch 222/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4357 - acc: 0.7847 - val_loss: 0.5116 - val_acc: 0.7552\n",
      "Epoch 223/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4356 - acc: 0.7847 - val_loss: 0.5116 - val_acc: 0.7552\n",
      "Epoch 224/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4356 - acc: 0.7847 - val_loss: 0.5116 - val_acc: 0.7552\n",
      "Epoch 225/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4355 - acc: 0.7847 - val_loss: 0.5116 - val_acc: 0.7552\n",
      "Epoch 226/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4355 - acc: 0.7847 - val_loss: 0.5116 - val_acc: 0.7552\n",
      "Epoch 227/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4355 - acc: 0.7847 - val_loss: 0.5116 - val_acc: 0.7552\n",
      "Epoch 228/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4354 - acc: 0.7865 - val_loss: 0.5116 - val_acc: 0.7552\n",
      "Epoch 229/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4354 - acc: 0.7847 - val_loss: 0.5116 - val_acc: 0.7552\n",
      "Epoch 230/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3814 - acc: 0.812 - 0s - loss: 0.4354 - acc: 0.7847 - val_loss: 0.5116 - val_acc: 0.7552\n",
      "Epoch 231/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4353 - acc: 0.7847 - val_loss: 0.5116 - val_acc: 0.7552\n",
      "Epoch 232/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4353 - acc: 0.7865 - val_loss: 0.5117 - val_acc: 0.7552\n",
      "Epoch 233/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4353 - acc: 0.7865 - val_loss: 0.5117 - val_acc: 0.7552\n",
      "Epoch 234/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4352 - acc: 0.7865 - val_loss: 0.5117 - val_acc: 0.7552\n",
      "Epoch 235/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4352 - acc: 0.7882 - val_loss: 0.5117 - val_acc: 0.7552\n",
      "Epoch 236/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4351 - acc: 0.7882 - val_loss: 0.5117 - val_acc: 0.7552\n",
      "Epoch 237/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4351 - acc: 0.7882 - val_loss: 0.5117 - val_acc: 0.7552\n",
      "Epoch 238/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4350 - acc: 0.7882 - val_loss: 0.5117 - val_acc: 0.7552\n",
      "Epoch 239/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4350 - acc: 0.7882 - val_loss: 0.5117 - val_acc: 0.7552\n",
      "Epoch 240/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4349 - acc: 0.7882 - val_loss: 0.5118 - val_acc: 0.7552\n",
      "Epoch 241/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4349 - acc: 0.7882 - val_loss: 0.5118 - val_acc: 0.7552\n",
      "Epoch 242/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4349 - acc: 0.7882 - val_loss: 0.5118 - val_acc: 0.7552\n",
      "Epoch 243/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4349 - acc: 0.7882 - val_loss: 0.5118 - val_acc: 0.7552\n",
      "Epoch 244/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4348 - acc: 0.7882 - val_loss: 0.5118 - val_acc: 0.7552\n",
      "Epoch 245/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4348 - acc: 0.7882 - val_loss: 0.5118 - val_acc: 0.7552\n",
      "Epoch 246/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4347 - acc: 0.7882 - val_loss: 0.5119 - val_acc: 0.7552\n",
      "Epoch 247/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4347 - acc: 0.7882 - val_loss: 0.5119 - val_acc: 0.7552\n",
      "Epoch 248/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4347 - acc: 0.7882 - val_loss: 0.5119 - val_acc: 0.7552\n",
      "Epoch 249/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4346 - acc: 0.7882 - val_loss: 0.5119 - val_acc: 0.7552\n",
      "Epoch 250/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4346 - acc: 0.7882 - val_loss: 0.5119 - val_acc: 0.7552\n",
      "Epoch 251/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4345 - acc: 0.7882 - val_loss: 0.5119 - val_acc: 0.7500\n",
      "Epoch 252/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4345 - acc: 0.7882 - val_loss: 0.5119 - val_acc: 0.7500\n",
      "Epoch 253/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4345 - acc: 0.7882 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 254/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4344 - acc: 0.7882 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 255/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4344 - acc: 0.7882 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 256/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4343 - acc: 0.7882 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 257/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s - loss: 0.4343 - acc: 0.7882 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 258/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4343 - acc: 0.7882 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 259/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4342 - acc: 0.7882 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 260/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4342 - acc: 0.7899 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 261/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4342 - acc: 0.7882 - val_loss: 0.5121 - val_acc: 0.7552\n",
      "Epoch 262/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4342 - acc: 0.7882 - val_loss: 0.5121 - val_acc: 0.7552\n",
      "Epoch 263/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4341 - acc: 0.7882 - val_loss: 0.5121 - val_acc: 0.7552\n",
      "Epoch 264/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4340 - acc: 0.7882 - val_loss: 0.5121 - val_acc: 0.7552\n",
      "Epoch 265/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4340 - acc: 0.7882 - val_loss: 0.5121 - val_acc: 0.7552\n",
      "Epoch 266/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4340 - acc: 0.7882 - val_loss: 0.5121 - val_acc: 0.7552\n",
      "Epoch 267/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4340 - acc: 0.7899 - val_loss: 0.5121 - val_acc: 0.7552\n",
      "Epoch 268/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4339 - acc: 0.7899 - val_loss: 0.5121 - val_acc: 0.7552\n",
      "Epoch 269/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4339 - acc: 0.7899 - val_loss: 0.5121 - val_acc: 0.7552\n",
      "Epoch 270/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4339 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7552\n",
      "Epoch 271/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4338 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7552\n",
      "Epoch 272/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4338 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7552\n",
      "Epoch 273/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.2726 - acc: 0.875 - 0s - loss: 0.4337 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7552\n",
      "Epoch 274/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4337 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7552\n",
      "Epoch 275/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4337 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7552\n",
      "Epoch 276/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4336 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7552\n",
      "Epoch 277/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4336 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7552\n",
      "Epoch 278/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4335 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7552\n",
      "Epoch 279/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4335 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7552\n",
      "Epoch 280/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4335 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 281/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4334 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 282/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4334 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 283/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4334 - acc: 0.7899 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 284/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4334 - acc: 0.7899 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 285/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4333 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 286/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4333 - acc: 0.7899 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 287/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4332 - acc: 0.7899 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 288/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4332 - acc: 0.7899 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 289/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4332 - acc: 0.7899 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 290/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4331 - acc: 0.7899 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 291/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4331 - acc: 0.7899 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 292/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4331 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 293/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4330 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 294/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4330 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 295/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4330 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 296/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4329 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 297/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4329 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 298/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4329 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 299/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4328 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 300/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4328 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 301/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4328 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 302/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4327 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 303/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4327 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 304/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4327 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 305/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4326 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 306/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4326 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 307/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4326 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 308/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4325 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 309/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4325 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 310/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4325 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 311/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4324 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 312/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4324 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 313/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4323 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 314/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4323 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 315/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4323 - acc: 0.7899 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 316/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4323 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 317/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4322 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 318/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4321 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 319/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4321 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 320/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4321 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 321/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4320 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 322/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4320 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 323/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4320 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 324/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4320 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 325/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4319 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 326/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4319 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 327/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4319 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 328/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4318 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 329/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4318 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 330/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4318 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 331/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4317 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 332/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4317 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 333/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4316 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 334/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4316 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 335/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4316 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 336/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4316 - acc: 0.7917 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 337/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4316 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 338/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4315 - acc: 0.7917 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 339/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4315 - acc: 0.7917 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 340/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4314 - acc: 0.7917 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 341/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4314 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 342/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4313 - acc: 0.7917 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 343/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4313 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 344/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4313 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7500\n",
      "Epoch 345/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4313 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 346/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4312 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 347/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4312 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 348/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4312 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 349/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4312 - acc: 0.7917 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 350/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4311 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 351/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4311 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 352/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4310 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 353/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4310 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 354/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4310 - acc: 0.7917 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 355/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4309 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 356/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4309 - acc: 0.7934 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 357/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3900 - acc: 0.843 - 0s - loss: 0.4309 - acc: 0.7917 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 358/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4309 - acc: 0.7934 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 359/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4308 - acc: 0.7934 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 360/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4308 - acc: 0.7917 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 361/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4308 - acc: 0.7934 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 362/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4307 - acc: 0.7917 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 363/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4307 - acc: 0.7934 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 364/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4307 - acc: 0.7951 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 365/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4306 - acc: 0.7951 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 366/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4306 - acc: 0.7934 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 367/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4306 - acc: 0.7934 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 368/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4305 - acc: 0.7951 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 369/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4305 - acc: 0.7951 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 370/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4305 - acc: 0.7951 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 371/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4304 - acc: 0.7951 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 372/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4304 - acc: 0.7917 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 373/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4304 - acc: 0.7951 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 374/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4304 - acc: 0.7951 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 375/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4303 - acc: 0.7951 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 376/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4303 - acc: 0.7951 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 377/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4303 - acc: 0.7951 - val_loss: 0.5125 - val_acc: 0.7552\n",
      "Epoch 378/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4303 - acc: 0.7951 - val_loss: 0.5125 - val_acc: 0.7552\n",
      "Epoch 379/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4302 - acc: 0.7951 - val_loss: 0.5125 - val_acc: 0.7552\n",
      "Epoch 380/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4302 - acc: 0.7951 - val_loss: 0.5125 - val_acc: 0.7552\n",
      "Epoch 381/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4301 - acc: 0.7951 - val_loss: 0.5125 - val_acc: 0.7552\n",
      "Epoch 382/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4301 - acc: 0.7951 - val_loss: 0.5125 - val_acc: 0.7552\n",
      "Epoch 383/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4301 - acc: 0.7951 - val_loss: 0.5125 - val_acc: 0.7552\n",
      "Epoch 384/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4301 - acc: 0.7951 - val_loss: 0.5125 - val_acc: 0.7552\n",
      "Epoch 385/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s - loss: 0.4300 - acc: 0.7969 - val_loss: 0.5125 - val_acc: 0.7552\n",
      "Epoch 386/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4300 - acc: 0.7969 - val_loss: 0.5125 - val_acc: 0.7552\n",
      "Epoch 387/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4300 - acc: 0.7951 - val_loss: 0.5125 - val_acc: 0.7552\n",
      "Epoch 388/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4300 - acc: 0.7969 - val_loss: 0.5126 - val_acc: 0.7552\n",
      "Epoch 389/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4299 - acc: 0.7969 - val_loss: 0.5126 - val_acc: 0.7552\n",
      "Epoch 390/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4299 - acc: 0.7969 - val_loss: 0.5126 - val_acc: 0.7552\n",
      "Epoch 391/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4299 - acc: 0.7969 - val_loss: 0.5126 - val_acc: 0.7552\n",
      "Epoch 392/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4299 - acc: 0.7969 - val_loss: 0.5126 - val_acc: 0.7552\n",
      "Epoch 393/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4299 - acc: 0.7969 - val_loss: 0.5126 - val_acc: 0.7552\n",
      "Epoch 394/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4298 - acc: 0.7969 - val_loss: 0.5126 - val_acc: 0.7552\n",
      "Epoch 395/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4298 - acc: 0.7969 - val_loss: 0.5126 - val_acc: 0.7552\n",
      "Epoch 396/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4298 - acc: 0.7969 - val_loss: 0.5126 - val_acc: 0.7552\n",
      "Epoch 397/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4297 - acc: 0.7969 - val_loss: 0.5127 - val_acc: 0.7552\n",
      "Epoch 398/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4298 - acc: 0.7969 - val_loss: 0.5127 - val_acc: 0.7552\n",
      "Epoch 399/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4297 - acc: 0.7969 - val_loss: 0.5127 - val_acc: 0.7552\n",
      "Epoch 400/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4296 - acc: 0.7969 - val_loss: 0.5127 - val_acc: 0.7552\n",
      "Epoch 401/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4296 - acc: 0.7969 - val_loss: 0.5127 - val_acc: 0.7552\n",
      "Epoch 402/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4296 - acc: 0.7969 - val_loss: 0.5127 - val_acc: 0.7552\n",
      "Epoch 403/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4296 - acc: 0.7969 - val_loss: 0.5127 - val_acc: 0.7552\n",
      "Epoch 404/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4296 - acc: 0.7969 - val_loss: 0.5127 - val_acc: 0.7552\n",
      "Epoch 405/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4296 - acc: 0.7986 - val_loss: 0.5127 - val_acc: 0.7552\n",
      "Epoch 406/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4295 - acc: 0.7969 - val_loss: 0.5127 - val_acc: 0.7552\n",
      "Epoch 407/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4295 - acc: 0.7986 - val_loss: 0.5128 - val_acc: 0.7552\n",
      "Epoch 408/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4294 - acc: 0.7986 - val_loss: 0.5128 - val_acc: 0.7552\n",
      "Epoch 409/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4294 - acc: 0.7969 - val_loss: 0.5128 - val_acc: 0.7552\n",
      "Epoch 410/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4294 - acc: 0.7986 - val_loss: 0.5128 - val_acc: 0.7552\n",
      "Epoch 411/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4294 - acc: 0.7986 - val_loss: 0.5128 - val_acc: 0.7552\n",
      "Epoch 412/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4293 - acc: 0.7986 - val_loss: 0.5128 - val_acc: 0.7552\n",
      "Epoch 413/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4293 - acc: 0.7986 - val_loss: 0.5128 - val_acc: 0.7552\n",
      "Epoch 414/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4293 - acc: 0.7986 - val_loss: 0.5128 - val_acc: 0.7552\n",
      "Epoch 415/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4293 - acc: 0.7986 - val_loss: 0.5128 - val_acc: 0.7552\n",
      "Epoch 416/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4293 - acc: 0.7986 - val_loss: 0.5128 - val_acc: 0.7552\n",
      "Epoch 417/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4292 - acc: 0.7986 - val_loss: 0.5128 - val_acc: 0.7552\n",
      "Epoch 418/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4292 - acc: 0.7986 - val_loss: 0.5129 - val_acc: 0.7552\n",
      "Epoch 419/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4292 - acc: 0.7986 - val_loss: 0.5129 - val_acc: 0.7552\n",
      "Epoch 420/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4292 - acc: 0.7986 - val_loss: 0.5129 - val_acc: 0.7552\n",
      "Epoch 421/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4292 - acc: 0.7986 - val_loss: 0.5129 - val_acc: 0.7552\n",
      "Epoch 422/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4291 - acc: 0.7986 - val_loss: 0.5129 - val_acc: 0.7552\n",
      "Epoch 423/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4291 - acc: 0.7986 - val_loss: 0.5129 - val_acc: 0.7552\n",
      "Epoch 424/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4291 - acc: 0.7986 - val_loss: 0.5129 - val_acc: 0.7552\n",
      "Epoch 425/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4290 - acc: 0.7986 - val_loss: 0.5129 - val_acc: 0.7552\n",
      "Epoch 426/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4290 - acc: 0.7986 - val_loss: 0.5129 - val_acc: 0.7552\n",
      "Epoch 427/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4290 - acc: 0.7986 - val_loss: 0.5129 - val_acc: 0.7552\n",
      "Epoch 428/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4290 - acc: 0.7986 - val_loss: 0.5129 - val_acc: 0.7552\n",
      "Epoch 429/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4289 - acc: 0.7986 - val_loss: 0.5130 - val_acc: 0.7552\n",
      "Epoch 430/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4289 - acc: 0.7986 - val_loss: 0.5130 - val_acc: 0.7552\n",
      "Epoch 431/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4289 - acc: 0.7986 - val_loss: 0.5130 - val_acc: 0.7552\n",
      "Epoch 432/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4289 - acc: 0.7986 - val_loss: 0.5130 - val_acc: 0.7552\n",
      "Epoch 433/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4289 - acc: 0.7986 - val_loss: 0.5130 - val_acc: 0.7552\n",
      "Epoch 434/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4289 - acc: 0.7986 - val_loss: 0.5130 - val_acc: 0.7552\n",
      "Epoch 435/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4288 - acc: 0.7986 - val_loss: 0.5130 - val_acc: 0.7552\n",
      "Epoch 436/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4288 - acc: 0.7986 - val_loss: 0.5130 - val_acc: 0.7552\n",
      "Epoch 437/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4288 - acc: 0.7986 - val_loss: 0.5130 - val_acc: 0.7552\n",
      "Epoch 438/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4287 - acc: 0.7986 - val_loss: 0.5130 - val_acc: 0.7552\n",
      "Epoch 439/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4287 - acc: 0.7986 - val_loss: 0.5130 - val_acc: 0.7552\n",
      "Epoch 440/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4287 - acc: 0.7986 - val_loss: 0.5130 - val_acc: 0.7552\n",
      "Epoch 441/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4286 - acc: 0.7986 - val_loss: 0.5130 - val_acc: 0.7552\n",
      "Epoch 442/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4286 - acc: 0.7986 - val_loss: 0.5130 - val_acc: 0.7552\n",
      "Epoch 443/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4286 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 444/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4286 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 445/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4286 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 446/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4286 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 447/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4285 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 448/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4285 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 449/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4285 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 450/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4285 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 451/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4284 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 452/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4284 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 453/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4284 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 454/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4284 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 455/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4283 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 456/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4283 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 457/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4283 - acc: 0.7986 - val_loss: 0.5131 - val_acc: 0.7552\n",
      "Epoch 458/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4283 - acc: 0.7986 - val_loss: 0.5132 - val_acc: 0.7552\n",
      "Epoch 459/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4282 - acc: 0.7986 - val_loss: 0.5132 - val_acc: 0.7552\n",
      "Epoch 460/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4282 - acc: 0.7986 - val_loss: 0.5132 - val_acc: 0.7552\n",
      "Epoch 461/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4282 - acc: 0.7986 - val_loss: 0.5132 - val_acc: 0.7552\n",
      "Epoch 462/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4282 - acc: 0.7986 - val_loss: 0.5132 - val_acc: 0.7552\n",
      "Epoch 463/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4281 - acc: 0.7986 - val_loss: 0.5132 - val_acc: 0.7552\n",
      "Epoch 464/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4281 - acc: 0.7969 - val_loss: 0.5132 - val_acc: 0.7552\n",
      "Epoch 465/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4281 - acc: 0.7969 - val_loss: 0.5132 - val_acc: 0.7552\n",
      "Epoch 466/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4281 - acc: 0.7969 - val_loss: 0.5132 - val_acc: 0.7552\n",
      "Epoch 467/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4280 - acc: 0.7969 - val_loss: 0.5132 - val_acc: 0.7552\n",
      "Epoch 468/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4280 - acc: 0.7969 - val_loss: 0.5132 - val_acc: 0.7552\n",
      "Epoch 469/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4280 - acc: 0.7969 - val_loss: 0.5132 - val_acc: 0.7552\n",
      "Epoch 470/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4280 - acc: 0.7969 - val_loss: 0.5132 - val_acc: 0.7552\n",
      "Epoch 471/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4280 - acc: 0.7969 - val_loss: 0.5133 - val_acc: 0.7552\n",
      "Epoch 472/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4279 - acc: 0.7969 - val_loss: 0.5133 - val_acc: 0.7552\n",
      "Epoch 473/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4279 - acc: 0.7969 - val_loss: 0.5133 - val_acc: 0.7552\n",
      "Epoch 474/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4279 - acc: 0.7969 - val_loss: 0.5133 - val_acc: 0.7552\n",
      "Epoch 475/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4278 - acc: 0.7969 - val_loss: 0.5133 - val_acc: 0.7552\n",
      "Epoch 476/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4278 - acc: 0.7969 - val_loss: 0.5133 - val_acc: 0.7552\n",
      "Epoch 477/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4278 - acc: 0.7969 - val_loss: 0.5133 - val_acc: 0.7552\n",
      "Epoch 478/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4278 - acc: 0.7969 - val_loss: 0.5133 - val_acc: 0.7552\n",
      "Epoch 479/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4278 - acc: 0.7969 - val_loss: 0.5133 - val_acc: 0.7552\n",
      "Epoch 480/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4277 - acc: 0.7969 - val_loss: 0.5133 - val_acc: 0.7552\n",
      "Epoch 481/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4277 - acc: 0.7969 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 482/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4277 - acc: 0.7986 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 483/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4276 - acc: 0.7969 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 484/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4276 - acc: 0.7969 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 485/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4276 - acc: 0.7969 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 486/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4276 - acc: 0.7986 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 487/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4275 - acc: 0.7986 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 488/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4275 - acc: 0.7986 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 489/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4275 - acc: 0.7969 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 490/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4275 - acc: 0.7986 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 491/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4275 - acc: 0.7986 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 492/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4274 - acc: 0.7986 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 493/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4274 - acc: 0.7986 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 494/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4274 - acc: 0.7986 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 495/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4274 - acc: 0.7986 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 496/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4274 - acc: 0.7986 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 497/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4273 - acc: 0.7986 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 498/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4273 - acc: 0.7986 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 499/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4273 - acc: 0.7986 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 500/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4272 - acc: 0.7986 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 501/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4272 - acc: 0.7986 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 502/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4272 - acc: 0.7986 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 503/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4272 - acc: 0.7986 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 504/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4272 - acc: 0.7969 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 505/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4271 - acc: 0.7986 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 506/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4271 - acc: 0.7986 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 507/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4271 - acc: 0.7969 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 508/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4271 - acc: 0.7969 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 509/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4271 - acc: 0.7969 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 510/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4270 - acc: 0.7969 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 511/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4270 - acc: 0.7969 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 512/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4270 - acc: 0.7969 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 513/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s - loss: 0.4270 - acc: 0.7969 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 514/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4270 - acc: 0.7969 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 515/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4269 - acc: 0.7969 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 516/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4269 - acc: 0.7969 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 517/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4269 - acc: 0.7969 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 518/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4268 - acc: 0.7969 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 519/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4268 - acc: 0.7969 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 520/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4268 - acc: 0.7969 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 521/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4268 - acc: 0.7969 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 522/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4268 - acc: 0.7969 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 523/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4267 - acc: 0.7969 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 524/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4267 - acc: 0.7969 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 525/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4267 - acc: 0.7969 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 526/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4267 - acc: 0.7969 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 527/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4266 - acc: 0.7969 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 528/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4266 - acc: 0.7969 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 529/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4266 - acc: 0.7969 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 530/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4266 - acc: 0.7969 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 531/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4266 - acc: 0.7969 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 532/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4266 - acc: 0.7969 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 533/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4265 - acc: 0.7969 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 534/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4265 - acc: 0.7969 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 535/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4265 - acc: 0.7969 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 536/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4265 - acc: 0.7969 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 537/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4265 - acc: 0.7969 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 538/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4264 - acc: 0.7969 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 539/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4264 - acc: 0.7969 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 540/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4264 - acc: 0.7969 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 541/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4263 - acc: 0.7969 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 542/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4263 - acc: 0.7969 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 543/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4263 - acc: 0.7969 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 544/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4263 - acc: 0.7969 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 545/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4263 - acc: 0.7969 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 546/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4262 - acc: 0.7969 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 547/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4262 - acc: 0.7969 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 548/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4262 - acc: 0.7969 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 549/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4262 - acc: 0.7969 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 550/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4261 - acc: 0.7969 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 551/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4261 - acc: 0.7969 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 552/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4261 - acc: 0.7969 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 553/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4261 - acc: 0.7969 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 554/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4261 - acc: 0.7969 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 555/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4260 - acc: 0.7969 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 556/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4260 - acc: 0.7969 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 557/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4260 - acc: 0.7969 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 558/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4260 - acc: 0.7969 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 559/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4259 - acc: 0.7969 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 560/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4259 - acc: 0.7969 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 561/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4259 - acc: 0.7969 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 562/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4259 - acc: 0.7969 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 563/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4258 - acc: 0.7969 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 564/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4258 - acc: 0.7969 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 565/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4258 - acc: 0.7969 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 566/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4258 - acc: 0.7986 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 567/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4257 - acc: 0.7986 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 568/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4257 - acc: 0.7969 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 569/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4257 - acc: 0.7986 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 570/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4257 - acc: 0.7986 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 571/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4257 - acc: 0.7969 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 572/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4256 - acc: 0.7969 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 573/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4256 - acc: 0.7986 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 574/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4256 - acc: 0.7986 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 575/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4255 - acc: 0.7969 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 576/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4255 - acc: 0.7986 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 577/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4255 - acc: 0.7986 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 578/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4255 - acc: 0.7986 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 579/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4255 - acc: 0.7986 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 580/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4254 - acc: 0.7986 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 581/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4254 - acc: 0.7986 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 582/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4253 - acc: 0.7986 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 583/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4254 - acc: 0.7986 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 584/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4253 - acc: 0.7986 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 585/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4253 - acc: 0.7986 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 586/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4253 - acc: 0.7986 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 587/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4252 - acc: 0.7986 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 588/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4252 - acc: 0.7986 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 589/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4252 - acc: 0.7986 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 590/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4252 - acc: 0.7986 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 591/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4252 - acc: 0.7986 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 592/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4251 - acc: 0.7986 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 593/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4251 - acc: 0.7986 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 594/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4251 - acc: 0.7986 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 595/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4251 - acc: 0.7986 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 596/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4250 - acc: 0.8003 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 597/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4250 - acc: 0.8003 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 598/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4250 - acc: 0.8003 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 599/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4249 - acc: 0.8003 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 600/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4250 - acc: 0.8021 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 601/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4249 - acc: 0.8021 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 602/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4249 - acc: 0.8021 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 603/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4249 - acc: 0.8021 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 604/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4248 - acc: 0.8021 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 605/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4248 - acc: 0.8021 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 606/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4248 - acc: 0.8021 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 607/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4248 - acc: 0.8021 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 608/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4247 - acc: 0.8021 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 609/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4248 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 610/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4247 - acc: 0.8021 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 611/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4247 - acc: 0.8021 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 612/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4247 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 613/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4246 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 614/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4246 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 615/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4246 - acc: 0.8021 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 616/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4246 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 617/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4245 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 618/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4245 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 619/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4245 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 620/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4245 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 621/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4245 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 622/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 623/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 624/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 625/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 626/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 627/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 628/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 629/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 630/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4242 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 631/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4242 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 632/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4242 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 633/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4242 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 634/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4241 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 635/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4241 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 636/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4241 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 637/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4241 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 638/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4240 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 639/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4240 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 640/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3322 - acc: 0.875 - 0s - loss: 0.4240 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 641/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s - loss: 0.4239 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 642/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4239 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 643/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4239 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 644/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4239 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 645/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4239 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 646/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4238 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 647/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4238 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7500\n",
      "Epoch 648/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4238 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 649/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4238 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7500\n",
      "Epoch 650/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4237 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7500\n",
      "Epoch 651/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4237 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7500\n",
      "Epoch 652/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4237 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7500\n",
      "Epoch 653/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4236 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7500\n",
      "Epoch 654/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4236 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 655/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4236 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7500\n",
      "Epoch 656/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4236 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7500\n",
      "Epoch 657/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4236 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7500\n",
      "Epoch 658/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4236 - acc: 0.8038 - val_loss: 0.5144 - val_acc: 0.7500\n",
      "Epoch 659/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4235 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 660/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4235 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 661/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4235 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 662/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4234 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 663/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4234 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 664/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4233 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 665/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4233 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 666/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4233 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 667/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4233 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 668/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4232 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 669/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4232 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 670/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4232 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 671/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4232 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 672/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4232 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 673/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4231 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 674/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4231 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 675/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4230 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 676/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4231 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 677/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4230 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 678/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4230 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 679/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4229 - acc: 0.8038 - val_loss: 0.5143 - val_acc: 0.7500\n",
      "Epoch 680/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4229 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 681/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4229 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 682/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4229 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 683/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4228 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 684/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4228 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 685/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4228 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 686/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4228 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 687/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4228 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 688/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4227 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 689/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4227 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 690/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4227 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 691/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4226 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 692/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4226 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 693/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4226 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 694/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4225 - acc: 0.8038 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 695/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4225 - acc: 0.8038 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 696/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4225 - acc: 0.8038 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 697/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4225 - acc: 0.8038 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 698/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 699/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 700/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4224 - acc: 0.8056 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 701/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 702/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4223 - acc: 0.8038 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 703/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4223 - acc: 0.8038 - val_loss: 0.5141 - val_acc: 0.7500\n",
      "Epoch 704/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4223 - acc: 0.8038 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 705/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4222 - acc: 0.8038 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 706/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4222 - acc: 0.8056 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 707/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4222 - acc: 0.8056 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 708/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4222 - acc: 0.8056 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 709/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4221 - acc: 0.8056 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 710/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4221 - acc: 0.8056 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 711/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4221 - acc: 0.8038 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 712/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4221 - acc: 0.8056 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 713/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4220 - acc: 0.8056 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 714/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4220 - acc: 0.8056 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 715/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4220 - acc: 0.8056 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 716/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4219 - acc: 0.8056 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 717/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4219 - acc: 0.8056 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 718/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4219 - acc: 0.8038 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 719/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4219 - acc: 0.8056 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 720/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4219 - acc: 0.8038 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 721/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4218 - acc: 0.8038 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 722/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4218 - acc: 0.8056 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 723/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4218 - acc: 0.8038 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 724/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4217 - acc: 0.8038 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 725/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4217 - acc: 0.8038 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 726/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4217 - acc: 0.8021 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 727/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4217 - acc: 0.8038 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 728/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4216 - acc: 0.8021 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 729/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4216 - acc: 0.8056 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 730/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4216 - acc: 0.8021 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 731/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4216 - acc: 0.8003 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 732/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4215 - acc: 0.8021 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 733/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4215 - acc: 0.8038 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 734/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4215 - acc: 0.8021 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 735/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4215 - acc: 0.8021 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 736/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4215 - acc: 0.8056 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 737/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 738/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4214 - acc: 0.8021 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 739/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 740/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4213 - acc: 0.8021 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 741/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 742/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 743/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 744/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 745/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 746/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 747/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4212 - acc: 0.8038 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 748/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4212 - acc: 0.8056 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 749/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4211 - acc: 0.8056 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 750/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4211 - acc: 0.8038 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 751/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4211 - acc: 0.8056 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 752/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4211 - acc: 0.8056 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 753/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4210 - acc: 0.8056 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 754/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4210 - acc: 0.8056 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 755/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4210 - acc: 0.8056 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 756/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4209 - acc: 0.8056 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 757/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4209 - acc: 0.8056 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 758/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4209 - acc: 0.8038 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 759/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4209 - acc: 0.8056 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 760/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4208 - acc: 0.8038 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 761/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4208 - acc: 0.8056 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 762/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4208 - acc: 0.8038 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 763/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4208 - acc: 0.8056 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 764/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4208 - acc: 0.8038 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 765/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4207 - acc: 0.8038 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 766/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4207 - acc: 0.8038 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 767/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4207 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 768/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4207 - acc: 0.8038 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 769/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s - loss: 0.4206 - acc: 0.8003 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 770/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4206 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 771/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4206 - acc: 0.8038 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 772/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4206 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 773/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4205 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 774/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4205 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 775/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4205 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 776/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4204 - acc: 0.8003 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 777/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4205 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 778/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4204 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 779/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4204 - acc: 0.8021 - val_loss: 0.5133 - val_acc: 0.7500\n",
      "Epoch 780/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4204 - acc: 0.8003 - val_loss: 0.5133 - val_acc: 0.7500\n",
      "Epoch 781/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4204 - acc: 0.8021 - val_loss: 0.5133 - val_acc: 0.7500\n",
      "Epoch 782/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4203 - acc: 0.8038 - val_loss: 0.5133 - val_acc: 0.7500\n",
      "Epoch 783/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4203 - acc: 0.8003 - val_loss: 0.5133 - val_acc: 0.7500\n",
      "Epoch 784/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4203 - acc: 0.8038 - val_loss: 0.5133 - val_acc: 0.7500\n",
      "Epoch 785/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4202 - acc: 0.8038 - val_loss: 0.5133 - val_acc: 0.7500\n",
      "Epoch 786/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4202 - acc: 0.8021 - val_loss: 0.5133 - val_acc: 0.7500\n",
      "Epoch 787/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4202 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 788/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4202 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 789/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4202 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 790/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4201 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 791/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4201 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 792/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4201 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 793/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4201 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 794/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4200 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 795/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4200 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 796/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4200 - acc: 0.8038 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 797/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4200 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 798/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4200 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 799/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4199 - acc: 0.8073 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 800/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4199 - acc: 0.8038 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 801/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4199 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 802/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4199 - acc: 0.8021 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 803/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4199 - acc: 0.8056 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 804/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4198 - acc: 0.8056 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 805/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4198 - acc: 0.8056 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 806/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4198 - acc: 0.8038 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 807/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4198 - acc: 0.8021 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 808/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4198 - acc: 0.8021 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 809/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4197 - acc: 0.8056 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 810/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4197 - acc: 0.8073 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 811/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4197 - acc: 0.8073 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 812/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4197 - acc: 0.8038 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 813/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4197 - acc: 0.8056 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 814/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4196 - acc: 0.8073 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 815/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4196 - acc: 0.8056 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 816/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4196 - acc: 0.8073 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 817/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4196 - acc: 0.8056 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 818/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4196 - acc: 0.8073 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 819/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4195 - acc: 0.8073 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 820/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4195 - acc: 0.8073 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 821/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4195 - acc: 0.8073 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 822/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4195 - acc: 0.8073 - val_loss: 0.5137 - val_acc: 0.7552\n",
      "Epoch 823/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4194 - acc: 0.8073 - val_loss: 0.5137 - val_acc: 0.7552\n",
      "Epoch 824/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4194 - acc: 0.8073 - val_loss: 0.5137 - val_acc: 0.7552\n",
      "Epoch 825/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4194 - acc: 0.8073 - val_loss: 0.5137 - val_acc: 0.7552\n",
      "Epoch 826/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4194 - acc: 0.8073 - val_loss: 0.5137 - val_acc: 0.7552\n",
      "Epoch 827/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4193 - acc: 0.8073 - val_loss: 0.5137 - val_acc: 0.7552\n",
      "Epoch 828/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4194 - acc: 0.8073 - val_loss: 0.5137 - val_acc: 0.7552\n",
      "Epoch 829/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4193 - acc: 0.8073 - val_loss: 0.5137 - val_acc: 0.7552\n",
      "Epoch 830/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4193 - acc: 0.8073 - val_loss: 0.5138 - val_acc: 0.7552\n",
      "Epoch 831/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4193 - acc: 0.8073 - val_loss: 0.5138 - val_acc: 0.7552\n",
      "Epoch 832/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4193 - acc: 0.8073 - val_loss: 0.5138 - val_acc: 0.7552\n",
      "Epoch 833/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4193 - acc: 0.8073 - val_loss: 0.5138 - val_acc: 0.7552\n",
      "Epoch 834/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4193 - acc: 0.8073 - val_loss: 0.5138 - val_acc: 0.7552\n",
      "Epoch 835/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4192 - acc: 0.8073 - val_loss: 0.5138 - val_acc: 0.7552\n",
      "Epoch 836/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4192 - acc: 0.8073 - val_loss: 0.5139 - val_acc: 0.7552\n",
      "Epoch 837/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4192 - acc: 0.8073 - val_loss: 0.5139 - val_acc: 0.7552\n",
      "Epoch 838/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4192 - acc: 0.8073 - val_loss: 0.5139 - val_acc: 0.7552\n",
      "Epoch 839/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4191 - acc: 0.8073 - val_loss: 0.5139 - val_acc: 0.7552\n",
      "Epoch 840/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4191 - acc: 0.8073 - val_loss: 0.5139 - val_acc: 0.7552\n",
      "Epoch 841/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4191 - acc: 0.8073 - val_loss: 0.5139 - val_acc: 0.7552\n",
      "Epoch 842/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4191 - acc: 0.8073 - val_loss: 0.5139 - val_acc: 0.7552\n",
      "Epoch 843/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4191 - acc: 0.8073 - val_loss: 0.5140 - val_acc: 0.7552\n",
      "Epoch 844/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4191 - acc: 0.8073 - val_loss: 0.5140 - val_acc: 0.7552\n",
      "Epoch 845/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4191 - acc: 0.8073 - val_loss: 0.5140 - val_acc: 0.7552\n",
      "Epoch 846/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4190 - acc: 0.8073 - val_loss: 0.5140 - val_acc: 0.7552\n",
      "Epoch 847/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4190 - acc: 0.8073 - val_loss: 0.5140 - val_acc: 0.7552\n",
      "Epoch 848/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4190 - acc: 0.8073 - val_loss: 0.5140 - val_acc: 0.7552\n",
      "Epoch 849/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4190 - acc: 0.8073 - val_loss: 0.5140 - val_acc: 0.7552\n",
      "Epoch 850/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4190 - acc: 0.8073 - val_loss: 0.5141 - val_acc: 0.7552\n",
      "Epoch 851/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4190 - acc: 0.8073 - val_loss: 0.5141 - val_acc: 0.7552\n",
      "Epoch 852/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4189 - acc: 0.8073 - val_loss: 0.5141 - val_acc: 0.7552\n",
      "Epoch 853/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4189 - acc: 0.8073 - val_loss: 0.5141 - val_acc: 0.7552\n",
      "Epoch 854/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4189 - acc: 0.8073 - val_loss: 0.5141 - val_acc: 0.7552\n",
      "Epoch 855/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4189 - acc: 0.8073 - val_loss: 0.5141 - val_acc: 0.7552\n",
      "Epoch 856/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4189 - acc: 0.8073 - val_loss: 0.5141 - val_acc: 0.7552\n",
      "Epoch 857/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4188 - acc: 0.8073 - val_loss: 0.5141 - val_acc: 0.7552\n",
      "Epoch 858/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4188 - acc: 0.8073 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 859/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4188 - acc: 0.8073 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 860/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4188 - acc: 0.8073 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 861/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4188 - acc: 0.8073 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 862/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4188 - acc: 0.8073 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 863/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4187 - acc: 0.8073 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 864/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4187 - acc: 0.8073 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 865/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4187 - acc: 0.8073 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 866/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4187 - acc: 0.8073 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 867/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4187 - acc: 0.8073 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 868/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4186 - acc: 0.8073 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 869/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4186 - acc: 0.8073 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 870/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4186 - acc: 0.8073 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 871/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4186 - acc: 0.8073 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 872/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4185 - acc: 0.8073 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 873/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4185 - acc: 0.8073 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 874/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4185 - acc: 0.8073 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 875/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4185 - acc: 0.8073 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 876/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4185 - acc: 0.8073 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 877/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4185 - acc: 0.8073 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 878/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4185 - acc: 0.8073 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 879/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4184 - acc: 0.8073 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 880/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4184 - acc: 0.8073 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 881/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4184 - acc: 0.8073 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 882/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4184 - acc: 0.8073 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 883/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4184 - acc: 0.8073 - val_loss: 0.5145 - val_acc: 0.7552\n",
      "Epoch 884/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4183 - acc: 0.8073 - val_loss: 0.5145 - val_acc: 0.7552\n",
      "Epoch 885/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4183 - acc: 0.8073 - val_loss: 0.5145 - val_acc: 0.7552\n",
      "Epoch 886/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4183 - acc: 0.8073 - val_loss: 0.5145 - val_acc: 0.7552\n",
      "Epoch 887/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4183 - acc: 0.8073 - val_loss: 0.5145 - val_acc: 0.7552\n",
      "Epoch 888/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4183 - acc: 0.8073 - val_loss: 0.5145 - val_acc: 0.7552\n",
      "Epoch 889/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4183 - acc: 0.8073 - val_loss: 0.5146 - val_acc: 0.7552\n",
      "Epoch 890/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4183 - acc: 0.8073 - val_loss: 0.5146 - val_acc: 0.7552\n",
      "Epoch 891/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4182 - acc: 0.8073 - val_loss: 0.5146 - val_acc: 0.7552\n",
      "Epoch 892/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4182 - acc: 0.8073 - val_loss: 0.5146 - val_acc: 0.7552\n",
      "Epoch 893/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4182 - acc: 0.8073 - val_loss: 0.5146 - val_acc: 0.7552\n",
      "Epoch 894/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4182 - acc: 0.8073 - val_loss: 0.5146 - val_acc: 0.7552\n",
      "Epoch 895/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4182 - acc: 0.8073 - val_loss: 0.5147 - val_acc: 0.7552\n",
      "Epoch 896/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4181 - acc: 0.8073 - val_loss: 0.5147 - val_acc: 0.7552\n",
      "Epoch 897/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s - loss: 0.4181 - acc: 0.8073 - val_loss: 0.5147 - val_acc: 0.7552\n",
      "Epoch 898/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4181 - acc: 0.8073 - val_loss: 0.5147 - val_acc: 0.7552\n",
      "Epoch 899/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4181 - acc: 0.8073 - val_loss: 0.5147 - val_acc: 0.7552\n",
      "Epoch 900/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4181 - acc: 0.8073 - val_loss: 0.5148 - val_acc: 0.7552\n",
      "Epoch 901/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4181 - acc: 0.8073 - val_loss: 0.5148 - val_acc: 0.7552\n",
      "Epoch 902/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4180 - acc: 0.8073 - val_loss: 0.5148 - val_acc: 0.7552\n",
      "Epoch 903/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4180 - acc: 0.8073 - val_loss: 0.5148 - val_acc: 0.7552\n",
      "Epoch 904/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4180 - acc: 0.8073 - val_loss: 0.5149 - val_acc: 0.7552\n",
      "Epoch 905/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4180 - acc: 0.8073 - val_loss: 0.5149 - val_acc: 0.7552\n",
      "Epoch 906/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4180 - acc: 0.8073 - val_loss: 0.5149 - val_acc: 0.7552\n",
      "Epoch 907/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4179 - acc: 0.8073 - val_loss: 0.5149 - val_acc: 0.7552\n",
      "Epoch 908/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4179 - acc: 0.8073 - val_loss: 0.5149 - val_acc: 0.7552\n",
      "Epoch 909/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4179 - acc: 0.8073 - val_loss: 0.5149 - val_acc: 0.7552\n",
      "Epoch 910/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4179 - acc: 0.8090 - val_loss: 0.5150 - val_acc: 0.7552\n",
      "Epoch 911/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4179 - acc: 0.8073 - val_loss: 0.5150 - val_acc: 0.7552\n",
      "Epoch 912/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4179 - acc: 0.8073 - val_loss: 0.5150 - val_acc: 0.7552\n",
      "Epoch 913/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4178 - acc: 0.8073 - val_loss: 0.5150 - val_acc: 0.7552\n",
      "Epoch 914/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4178 - acc: 0.8073 - val_loss: 0.5150 - val_acc: 0.7552\n",
      "Epoch 915/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4178 - acc: 0.8073 - val_loss: 0.5151 - val_acc: 0.7552\n",
      "Epoch 916/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4178 - acc: 0.8090 - val_loss: 0.5151 - val_acc: 0.7552\n",
      "Epoch 917/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4178 - acc: 0.8073 - val_loss: 0.5151 - val_acc: 0.7552\n",
      "Epoch 918/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4177 - acc: 0.8073 - val_loss: 0.5151 - val_acc: 0.7552\n",
      "Epoch 919/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4177 - acc: 0.8073 - val_loss: 0.5151 - val_acc: 0.7552\n",
      "Epoch 920/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4177 - acc: 0.8090 - val_loss: 0.5151 - val_acc: 0.7552\n",
      "Epoch 921/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4176 - acc: 0.8090 - val_loss: 0.5152 - val_acc: 0.7552\n",
      "Epoch 922/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4176 - acc: 0.8090 - val_loss: 0.5152 - val_acc: 0.7552\n",
      "Epoch 923/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4176 - acc: 0.8090 - val_loss: 0.5152 - val_acc: 0.7552\n",
      "Epoch 924/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4176 - acc: 0.8090 - val_loss: 0.5152 - val_acc: 0.7552\n",
      "Epoch 925/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4176 - acc: 0.8090 - val_loss: 0.5153 - val_acc: 0.7552\n",
      "Epoch 926/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4175 - acc: 0.8090 - val_loss: 0.5153 - val_acc: 0.7552\n",
      "Epoch 927/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4175 - acc: 0.8090 - val_loss: 0.5153 - val_acc: 0.7552\n",
      "Epoch 928/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4175 - acc: 0.8090 - val_loss: 0.5153 - val_acc: 0.7552\n",
      "Epoch 929/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4175 - acc: 0.8073 - val_loss: 0.5153 - val_acc: 0.7552\n",
      "Epoch 930/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4175 - acc: 0.8073 - val_loss: 0.5154 - val_acc: 0.7552\n",
      "Epoch 931/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4175 - acc: 0.8090 - val_loss: 0.5154 - val_acc: 0.7552\n",
      "Epoch 932/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4174 - acc: 0.8090 - val_loss: 0.5154 - val_acc: 0.7552\n",
      "Epoch 933/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4174 - acc: 0.8090 - val_loss: 0.5154 - val_acc: 0.7552\n",
      "Epoch 934/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4174 - acc: 0.8073 - val_loss: 0.5154 - val_acc: 0.7552\n",
      "Epoch 935/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4174 - acc: 0.8073 - val_loss: 0.5155 - val_acc: 0.7552\n",
      "Epoch 936/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4174 - acc: 0.8073 - val_loss: 0.5155 - val_acc: 0.7552\n",
      "Epoch 937/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4173 - acc: 0.8073 - val_loss: 0.5155 - val_acc: 0.7552\n",
      "Epoch 938/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4173 - acc: 0.8073 - val_loss: 0.5155 - val_acc: 0.7552\n",
      "Epoch 939/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4173 - acc: 0.8073 - val_loss: 0.5155 - val_acc: 0.7552\n",
      "Epoch 940/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4173 - acc: 0.8073 - val_loss: 0.5155 - val_acc: 0.7552\n",
      "Epoch 941/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4173 - acc: 0.8073 - val_loss: 0.5156 - val_acc: 0.7552\n",
      "Epoch 942/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4173 - acc: 0.8073 - val_loss: 0.5156 - val_acc: 0.7552\n",
      "Epoch 943/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4172 - acc: 0.8090 - val_loss: 0.5156 - val_acc: 0.7552\n",
      "Epoch 944/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4172 - acc: 0.8073 - val_loss: 0.5156 - val_acc: 0.7552\n",
      "Epoch 945/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4172 - acc: 0.8073 - val_loss: 0.5157 - val_acc: 0.7552\n",
      "Epoch 946/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4172 - acc: 0.8073 - val_loss: 0.5157 - val_acc: 0.7552\n",
      "Epoch 947/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4172 - acc: 0.8073 - val_loss: 0.5157 - val_acc: 0.7552\n",
      "Epoch 948/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4171 - acc: 0.8073 - val_loss: 0.5157 - val_acc: 0.7552\n",
      "Epoch 949/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4171 - acc: 0.8073 - val_loss: 0.5157 - val_acc: 0.7552\n",
      "Epoch 950/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4171 - acc: 0.8090 - val_loss: 0.5157 - val_acc: 0.7552\n",
      "Epoch 951/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4171 - acc: 0.8073 - val_loss: 0.5158 - val_acc: 0.7552\n",
      "Epoch 952/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4171 - acc: 0.8073 - val_loss: 0.5158 - val_acc: 0.7552\n",
      "Epoch 953/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4170 - acc: 0.8073 - val_loss: 0.5158 - val_acc: 0.7552\n",
      "Epoch 954/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4170 - acc: 0.8090 - val_loss: 0.5158 - val_acc: 0.7552\n",
      "Epoch 955/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4170 - acc: 0.8090 - val_loss: 0.5158 - val_acc: 0.7552\n",
      "Epoch 956/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4170 - acc: 0.8090 - val_loss: 0.5159 - val_acc: 0.7552\n",
      "Epoch 957/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4170 - acc: 0.8073 - val_loss: 0.5159 - val_acc: 0.7552\n",
      "Epoch 958/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4169 - acc: 0.8090 - val_loss: 0.5159 - val_acc: 0.7552\n",
      "Epoch 959/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4169 - acc: 0.8073 - val_loss: 0.5159 - val_acc: 0.7552\n",
      "Epoch 960/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4169 - acc: 0.8090 - val_loss: 0.5159 - val_acc: 0.7552\n",
      "Epoch 961/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4169 - acc: 0.8090 - val_loss: 0.5160 - val_acc: 0.7552\n",
      "Epoch 962/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4169 - acc: 0.8090 - val_loss: 0.5160 - val_acc: 0.7552\n",
      "Epoch 963/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4169 - acc: 0.8090 - val_loss: 0.5160 - val_acc: 0.7552\n",
      "Epoch 964/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4168 - acc: 0.8090 - val_loss: 0.5160 - val_acc: 0.7552\n",
      "Epoch 965/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4168 - acc: 0.8090 - val_loss: 0.5160 - val_acc: 0.7552\n",
      "Epoch 966/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4168 - acc: 0.8073 - val_loss: 0.5160 - val_acc: 0.7552\n",
      "Epoch 967/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4168 - acc: 0.8073 - val_loss: 0.5161 - val_acc: 0.7552\n",
      "Epoch 968/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4168 - acc: 0.8108 - val_loss: 0.5161 - val_acc: 0.7552\n",
      "Epoch 969/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4167 - acc: 0.8108 - val_loss: 0.5161 - val_acc: 0.7552\n",
      "Epoch 970/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4167 - acc: 0.8108 - val_loss: 0.5161 - val_acc: 0.7552\n",
      "Epoch 971/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4167 - acc: 0.8090 - val_loss: 0.5161 - val_acc: 0.7552\n",
      "Epoch 972/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4167 - acc: 0.8090 - val_loss: 0.5162 - val_acc: 0.7552\n",
      "Epoch 973/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4167 - acc: 0.8090 - val_loss: 0.5162 - val_acc: 0.7552\n",
      "Epoch 974/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4167 - acc: 0.8090 - val_loss: 0.5162 - val_acc: 0.7552\n",
      "Epoch 975/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4166 - acc: 0.8090 - val_loss: 0.5162 - val_acc: 0.7552\n",
      "Epoch 976/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4166 - acc: 0.8090 - val_loss: 0.5162 - val_acc: 0.7552\n",
      "Epoch 977/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4166 - acc: 0.8090 - val_loss: 0.5162 - val_acc: 0.7552\n",
      "Epoch 978/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4166 - acc: 0.8090 - val_loss: 0.5163 - val_acc: 0.7552\n",
      "Epoch 979/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4166 - acc: 0.8090 - val_loss: 0.5163 - val_acc: 0.7552\n",
      "Epoch 980/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4166 - acc: 0.8090 - val_loss: 0.5163 - val_acc: 0.7552\n",
      "Epoch 981/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4166 - acc: 0.8090 - val_loss: 0.5163 - val_acc: 0.7552\n",
      "Epoch 982/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4165 - acc: 0.8108 - val_loss: 0.5163 - val_acc: 0.7552\n",
      "Epoch 983/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4165 - acc: 0.8090 - val_loss: 0.5164 - val_acc: 0.7552\n",
      "Epoch 984/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4165 - acc: 0.8108 - val_loss: 0.5164 - val_acc: 0.7552\n",
      "Epoch 985/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4165 - acc: 0.8090 - val_loss: 0.5164 - val_acc: 0.7552\n",
      "Epoch 986/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4165 - acc: 0.8108 - val_loss: 0.5164 - val_acc: 0.7552\n",
      "Epoch 987/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4164 - acc: 0.8090 - val_loss: 0.5164 - val_acc: 0.7552\n",
      "Epoch 988/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4164 - acc: 0.8090 - val_loss: 0.5164 - val_acc: 0.7552\n",
      "Epoch 989/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4164 - acc: 0.8090 - val_loss: 0.5165 - val_acc: 0.7552\n",
      "Epoch 990/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4164 - acc: 0.8090 - val_loss: 0.5165 - val_acc: 0.7552\n",
      "Epoch 991/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4164 - acc: 0.8090 - val_loss: 0.5165 - val_acc: 0.7552\n",
      "Epoch 992/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4163 - acc: 0.8090 - val_loss: 0.5165 - val_acc: 0.7552\n",
      "Epoch 993/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4164 - acc: 0.8090 - val_loss: 0.5165 - val_acc: 0.7552\n",
      "Epoch 994/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4163 - acc: 0.8090 - val_loss: 0.5165 - val_acc: 0.7552\n",
      "Epoch 995/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4163 - acc: 0.8090 - val_loss: 0.5166 - val_acc: 0.7552\n",
      "Epoch 996/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4163 - acc: 0.8090 - val_loss: 0.5166 - val_acc: 0.7552\n",
      "Epoch 997/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4163 - acc: 0.8090 - val_loss: 0.5166 - val_acc: 0.7552\n",
      "Epoch 998/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4163 - acc: 0.8108 - val_loss: 0.5166 - val_acc: 0.7552\n",
      "Epoch 999/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4162 - acc: 0.8108 - val_loss: 0.5166 - val_acc: 0.7552\n",
      "Epoch 1000/1000\n",
      "576/576 [==============================] - 0s - loss: 0.4162 - acc: 0.8090 - val_loss: 0.5167 - val_acc: 0.7552\n"
     ]
    }
   ],
   "source": [
    "## Note that when we call \"fit\" again, it picks up where it left off\n",
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c1c634080>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6sAAAHVCAYAAAAXVW0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkqTCMaxKkiRJkgrn3wFe98kpaMmk5AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1c44def0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training.  What is the appropriate number of epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Now it's your turn.  Do the following in the cells below:\n",
    "- Build a model with two hidden layers, each with 6 nodes\n",
    "- Use the \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer\n",
    "- Use a learning rate of .003 and train for 1500 epochs\n",
    "- Graph the trajectory of the loss functions, accuracy on both train and test set\n",
    "- Plot the roc curve for the predictions\n",
    "\n",
    "Experiment with different learning rates, numbers of epochs, and network structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
