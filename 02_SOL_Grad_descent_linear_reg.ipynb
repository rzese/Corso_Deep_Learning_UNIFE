{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_SOL_Grad_descent_linear_reg.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6LbQhIFmQoII","colab_type":"text"},"source":["## ML Review and Gradient Descent Example"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Ttm3CZQbQoIK","colab_type":"text"},"source":["In this notebook, we will solve a simple linear regression problem by gradient descent.  \n","We will see the effect of the learning rate on the trajectory in parameter space.\n","We will show how Stochastic Gradient Descent (SGD) differs from the standard version, and the effect of \"shuffling\" your data during SGD."]},{"cell_type":"code","metadata":{"id":"ZF4-TFpzQoIL","colab_type":"code","colab":{}},"source":["# Preliminaries - packages to load\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"93KUCAB6QoIP","colab_type":"text"},"source":["## Generate Data from a known distribution\n","Below we will generate data from a known distribution.  \n","Specifically, the true model is:\n","\n","$Y = b + \\theta_1 X_1 + \\theta_2 X_2 + \\epsilon$\n","\n","$X_1$ and $X_2$ have a uniform distribution on the interval $[0,10]$, while `const` is a vector of ones (representing the intercept term).\n","\n","We set actual values for $b$ ,$\\theta_1$, and $\\theta_2$\n","\n","Here $b=1.5$, $\\theta_1=2$, and $\\theta_2=5$\n","\n","We then generate a vector of $y$-values according to the model and put the predictors together in a \"feature matrix\" `x_mat`"]},{"cell_type":"code","metadata":{"id":"7yZH1fNPQoIQ","colab_type":"code","colab":{}},"source":["np.random.seed(1234)  ## This ensures we get the same data if all of the other parameters remain fixed\n","\n","num_obs = 100\n","x1 = np.random.uniform(0,10,num_obs)\n","x2 = np.random.uniform(0,10,num_obs)\n","const = np.ones(num_obs)\n","eps = np.random.normal(0,.5,num_obs)\n","\n","b = 1.5\n","theta_1 = 2\n","theta_2 = 5\n","\n","y0 = b*const+ theta_1*x1 + theta_2*x2 + eps # computes y0\n","\n","                                 #                              const\n","x_mat0 = np.array([const,x1,x2]) # creates a matrix containing   x1\n","                                 #                               x2\n","\n","                                              #                   y0.T\n","t = np.concatenate((np.array([y0.T]),x_mat0)) # creates a matrix  const\n","                                              #                    x1\n","                                              #                    x2\n","\n","t = t.T # creates a matrix where each row i contains y0[i], const[i], x1[i], x2[i]\n","\n","np.random.shuffle(t) # moves randomly the rows\n","y=t[:,0] # takes first column (0) from all rows\n","x_mat=t[:,1:4] # takes columns 1-4 from all rows\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a9nTD-NaQoIU","colab_type":"text"},"source":["## Get the \"Right\" answer directly\n","In the below cells we solve for the optimal set of coefficients.  Note that even though the true model is given by:\n","\n","$b=1.5$, $\\theta_1=2$, and $\\theta_2=5$\n","\n","The maximum likelihood (least-squares) estimate from a finite data set may be slightly different."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"_BtdHKIHQoIV","colab_type":"text"},"source":["## Exercise:\n","Solve the problem two ways: \n","1. By using the scikit-learn LinearRegression model\n","2. Using matrix algebra directly via the formula $\\theta = (X^T X)^{-1}X^Ty$\n","\n","Note: The scikit-learn solver may give a warning message, this can be ignored."]},{"cell_type":"code","metadata":{"id":"oleP2EAwQoIW","colab_type":"code","colab":{}},"source":["### Solve directly using sklearn\n","from sklearn.linear_model import LinearRegression\n","\n","lr_model = LinearRegression(fit_intercept=False) # ensures that b is calculated\n","lr_model.fit(x_mat, y)\n","\n","lr_model.coef_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EiyymT5sQoIe","colab_type":"code","colab":{}},"source":["## Solve by matrix calculation\n","np.linalg.inv(np.dot(x_mat.T,x_mat)).dot(x_mat.T).dot(y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RMD4-LCrQoIi","colab_type":"text"},"source":["## Solving by Gradient Descent\n","\n","\n","For most numerical problems, we don't / can't know the underlying analytical solution. This is because we only arrive at analytical solutions by solving the equations mathematically, with pen and paper. That is more often than not just impossible. Fortunately, we have a way of converging to an approximate solution, by using **Gradient Descent**.\n","\n","\n","We will explore this very useful method because Neural Networks, along with many other complicated algorithms, are trained using Gradient Descent.  Seeing how gradient descent works on a simple example will build intuition and help us understand some of the nuances around setting the learning rate and other parameters.  We will also explore Stochastic Gradient Descent and compare its behavior to the standard approach."]},{"cell_type":"markdown","metadata":{"id":"YPOuIemGQoIj","colab_type":"text"},"source":["## Exercise\n","\n","The next several cells have code to perform (full-batch) gradient descent.  We have omitted some parameters for you to fill in.\n","\n","1. Pick a learning rate, and a number of iterations, run the code, and then plot the trajectory of your gradient descent.\n","1. Find examples where the learning rate is too high, too low, and \"just right\".\n","1. Look at plots of loss function under these conditions.\n","\n"]},{"cell_type":"code","metadata":{"id":"Ii9CxGUiQoIk","colab_type":"code","colab":{}},"source":["## Parameters to play with \n","learning_rate = 3e-3\n","num_iter = 10000\n","theta_initial = np.array([3,3,3])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBdhH2y-QoIr","colab_type":"code","colab":{}},"source":["def gradient_descent(learning_rate, num_iter, theta_initial):\n","\n","    ## Initialization steps\n","    theta = theta_initial\n","    theta_path = np.zeros((num_iter+1,3))\n","    theta_path[0,:]= theta_initial\n","    \n","    loss_vec = np.zeros(num_iter)\n","\n","    ## Main Gradient Descent loop (for a fixed number of iterations)\n","    for i in range(num_iter):\n","        y_pred = np.dot(theta.T,x_mat.T)\n","        loss_vec[i] = np.sum((y-y_pred)**2)\n","        grad_vec = (y-y_pred).dot(x_mat)/num_obs  #sum up the gradients across all observations and divide by num_obs\n","        theta = theta + learning_rate*grad_vec\n","        theta_path[i+1,:]=theta\n","    return theta_path, loss_vec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ofy6qhjeQoIv","colab_type":"code","colab":{}},"source":["true_coef = [b, theta_1, theta_2]\n","\n","def plot_ij(theta_path, i, j, ax):\n","    ax.plot(true_coef[i], true_coef[j],\n","            marker='p', markersize=15, label='true coef', color='#778899')\n","    ax.plot(theta_path[:, i],theta_path[:, j],\n","            color='k', linestyle='--', marker='^', markersize=5, markevery=50)\n","    ax.plot(theta_path[0, i], theta_path[0, j], marker='d', markersize=15, label='start', color='#F08080')\n","    ax.plot(theta_path[-1, i], theta_path[-1, j], marker='o', markersize=15, label='finish', color='#F08080')\n","    ax.set(\n","        xlabel='theta'+str(i),\n","        ylabel='theta'+str(j))\n","    ax.axis('equal')\n","    ax.grid(True)\n","    ax.legend(loc='best')\n","    \n","\n","def plot_all(theta_path, loss_vec, learning_rate, num_iter, theta_initial, gdtype='Gradient Descent'):\n","    plot_col = [4,2] # nrows, n col\n","    fig = plt.figure(figsize=(16, 32))\n","    title = '{gdtype} in the 3d parameter space - Learning rate is {lr} // {iters} iters // starting point {initial}'\n","    title = title.format(gdtype=gdtype, lr=learning_rate, iters=num_iter, initial=theta_initial)\n","    fig.suptitle(title, fontsize=20)\n","    ax = fig.add_subplot(plot_col[0],plot_col[1], 1)\n","    plot_ij(theta_path, 0, 1, ax)\n","    ax = fig.add_subplot(plot_col[0],plot_col[1], 2)\n","    plot_ij(theta_path, 0, 2, ax)\n","    ax = fig.add_subplot(plot_col[0],plot_col[1], 3)\n","    plot_ij(theta_path, 1, 2, ax)\n","    ax = fig.add_subplot(plot_col[0],plot_col[1], 4)\n","    ax.plot(loss_vec)\n","    ax.set(xlabel='iterations', ylabel='squared loss')\n","    ax.grid(True)\n","    ax = fig.add_subplot(plot_col[0],plot_col[1], 5)\n","    ax.plot(theta_path[:,0])\n","    ax.set(xlabel='iterations', ylabel='b')\n","    ax.grid(True)\n","    ax = fig.add_subplot(plot_col[0],plot_col[1], 6)\n","    ax.plot(theta_path[:,1])\n","    ax.set(xlabel='iterations', ylabel='theta1')\n","    ax.grid(True)\n","    ax = fig.add_subplot(plot_col[0],plot_col[1], 7)\n","    ax.plot(theta_path[:,2])\n","    ax.set(xlabel='iterations', ylabel='theta2')\n","    ax.grid(True)\n","    \n","    \n","\n","theta_path, loss_vec = gradient_descent(learning_rate, num_iter, theta_initial)\n","plot_all(theta_path, loss_vec, learning_rate, num_iter, theta_initial)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AkIbx-75QoI1","colab_type":"text"},"source":["## Stochastic Gradient Descent\n","Rather than average the gradients across the whole dataset before taking a step, we will now take a step for every datapoint.  Each step will be somewhat of an \"overreaction\" but they should average out.  "]},{"cell_type":"markdown","metadata":{"id":"x7Iqbuq2QoI3","colab_type":"text"},"source":["## Exercise\n","The below code runs Stochastic Gradient descent, but runs through the data in the same order every time.  \n","\n","1. Run the code and plot the graphs.  What do you notice?\n","2. Modify the code so that it randomly re-orders the data.  How do the sample trajectories compare?"]},{"cell_type":"code","metadata":{"id":"C3wB0lftQoI5","colab_type":"code","colab":{}},"source":["def stochastic_gradient_descent(learning_rate, num_iter, theta_initial):\n","\n","    ## Initialization steps\n","    theta = theta_initial\n","    # below are different in STOCHASTIC gradient descent\n","    theta_path = np.zeros(((num_iter*num_obs)+1,3))\n","    theta_path[0,:] = theta_initial\n","    loss_vec = np.zeros(num_iter*num_obs)\n","\n","    ## Main SGD loop\n","    count = 0\n","    for i in range(num_iter):\n","        for j in range(num_obs):\n","            count+=1\n","            y_pred = np.dot(theta.T,x_mat.T)\n","            loss_vec[count-1] = np.sum((y-y_pred)**2)\n","            grad_vec = (y[j]-y_pred[j])*(x_mat[j,:])\n","            theta = theta + learning_rate*grad_vec\n","            theta_path[count,:]=theta\n","    return theta_path, loss_vec\n","\n","\n","def shuff_stochastic_gradient_descent(learning_rate, num_iter, theta_initial):\n","\n","    ## Initialization steps\n","    theta = theta_initial\n","    # below are different in STOCHASTIC gradient descent\n","    theta_path = np.zeros(((num_iter*num_obs)+1,3))\n","    theta_path[0,:] = theta_initial\n","    loss_vec = np.zeros(num_iter*num_obs)\n","\n","    ## Main SGD loop\n","    count = 0\n","    for i in range(num_iter):\n","        np.random.shuffle(t)    # x_mat and y are splices of t, by shuffling t we shuffle also them! \n","        for j in range(num_obs):\n","            count+=1\n","            y_pred = np.dot(theta.T,x_mat.T)\n","            loss_vec[count-1] = np.sum((y-y_pred)**2)\n","            grad_vec = (y[j]-y_pred[j])*(x_mat[j,:])\n","            theta = theta + learning_rate*grad_vec\n","            theta_path[count,:]=theta\n","    return theta_path, loss_vec\n","\n","## Parameters to play with\n","learning_rate = 3e-3\n","num_iter = 70\n","theta_initial = np.array([3, 3, 3])\n","\n","theta_path, loss_vec = stochastic_gradient_descent(learning_rate, num_iter, theta_initial)\n","plot_all(theta_path, loss_vec, learning_rate, num_iter, theta_initial, 'SGD')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nDKD8nCgQoI-","colab_type":"text"},"source":["Play with the parameters below and observe the trajectory it results in"]},{"cell_type":"code","metadata":{"id":"53o7rzCcQoI_","colab_type":"code","colab":{}},"source":["## Parameters to play with\n","learning_rate = 1e-3\n","num_iter = 100\n","theta_initial = np.array([3,3,3])\n","\n","\n","theta_path, loss_vec = shuff_stochastic_gradient_descent(learning_rate, num_iter, theta_initial)\n","plot_all(theta_path, loss_vec, learning_rate, num_iter, theta_initial, 'SGD')"],"execution_count":0,"outputs":[]}]}